<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Emboieded AI on Welcome to Bertsin Homepage!</title><link>https://user-xixiboliya.github.io/categories/emboieded-ai/</link><description>Recent content in Emboieded AI on Welcome to Bertsin Homepage!</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 21 Oct 2025 16:00:00 -0720</lastBuildDate><atom:link href="https://user-xixiboliya.github.io/categories/emboieded-ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Paper Reading 1</title><link>https://user-xixiboliya.github.io/post/paperreading/</link><pubDate>Tue, 21 Oct 2025 15:00:00 -0710</pubDate><guid>https://user-xixiboliya.github.io/post/paperreading/</guid><description>&lt;h1 id="pi0">
&lt;a class="header-anchor" href="#pi0">&lt;/a>
pi0
&lt;/h1>$$\boldsymbol{o}_t = \left[ \left[ \boldsymbol{I}_1^t, \dots, \boldsymbol{I}_n^t, \ell_t \right], \left[ \boldsymbol{q}_t \right], \left[ \boldsymbol{a}_t^\tau, \boldsymbol{a}_{t+1}^\tau, \dots, \boldsymbol{a}_{t+H-1}^\tau \right] \right]$$&lt;p>
&lt;img src="output_image/b4cdde6c0b4bfdc0e1809b25b4a2b740.png" alt="">&lt;/p>
&lt;p>$\pi_{0}$的数据集构建的很成功，预训练的数据来源未$\pi 0$数据集以及开源数据集OXE，以量取胜，之后再对于特定任务进行微调。为了使数据结构统一，robot state被对齐到18维度。&lt;/p>
$$L^\tau(\theta) = \mathbb{E}_{p(\boldsymbol{A}_t \mid \boldsymbol{o}_t), q(\boldsymbol{A}_t^\tau \mid \boldsymbol{A}_t)} \left\| \boldsymbol{v}_\theta \left( \boldsymbol{A}_t^\tau, \boldsymbol{o}_t \right) - \boldsymbol{u} \left( \boldsymbol{A}_t^\tau \mid \boldsymbol{A}_t \right) \right\|^2$$&lt;p>
在推理时，真实动作序列通过Euler步进法进行恢复,$\boldsymbol{A}_{t}^{\tau + \delta} = \boldsymbol{A}_{t}^{\tau} + \delta \boldsymbol{v}_{\theta}(\boldsymbol{A}_{t}^{\tau}, \boldsymbol{o}_{t})$，$\delta$=0.1。以下是对上式的解读，不过貌似原论文没找到。
&lt;img src="output_image/7531c5cd6f6e9000ed73ec099d3c1b4a.png" alt="">&lt;/p>
&lt;p>原文提出的一些局限性：大多数知识来源于预训练阶段，微调则负责告诉模型如何用某些知识完成用户的指令。复杂任务的高精度数据(longhorizon移动操作任务)进行微调会导致模型脆弱，以zero shot 方式运行预训练模型并不总能展现出后训练数据中所展示的流畅策略。&lt;/p>
&lt;blockquote>
&lt;p>action expert 有两种，一种是自回归的detokenizer,另一种是flow matching 的Diffusion Policy（例如pi0）。流匹配的DP倾向于让model学到一个分布，将噪声流到动作，输出的控制信号是连续的。自回归的Detokenizer是逐token预测，输出空间是离散的token化动作。&lt;/p>
&lt;p>对于VLA而言，大部分VLA的主干是VLM，另⼀种范式的兴起也不可忽视，也就是World Model的范式，换言之，即使用 Video Prediction Model 作为 VLA 的主干部分，并且辅佐以 DP 或者 Detokenizer 进行 Action 的输出。&lt;a href="https://axi404.top/blog/embodied-talk-2#vlm-%E7%9A%84%E4%B8%A4%E7%A7%8D%E8%8C%83%E5%BC%8F">参考链接&lt;/a> 相关工作有MindJourney 、Unified Video Action Model和Genie Envisioner。但这个idea并不是很新的了。&lt;/p></description></item></channel></rss>