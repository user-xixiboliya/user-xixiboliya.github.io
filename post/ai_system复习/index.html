<!doctype html><html lang=en-us data-theme-mode=auto><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>2025XJTU AI系统复习 | Bertsin
</title><meta name=description content="本课程的中文名称设定为人工智能系统，主要讲解支持人工智能的计算机系统设计，对应的英文课程名称为 System for AI。本课程中将交替使用以下词汇：人工智能系统，AI-System 和 System for AI。本课程为微软人工智能教育与共建社区中规划的人工智能相关教程之一，在基础教程模块下，课程编号和名称为 A6-人工智能系统。"><script>window.siteConfig=JSON.parse('{"anchor_icon":null,"clipboard":{"copyright":{"content":"本文版权：本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！","count":50,"enable":false},"fail":"复制失败 (ﾟ⊿ﾟ)ﾂ","success":"复制成功(*^▽^*)"},"code_block":{"expand":true},"icon_font":"4552607_tq6stt6tcg","outdate":{"daysago":180,"enable":false,"message":"本文最后更新于 {time}，请注意文中内容可能已经发生变化。"}}')</script><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7cNoto%20Serif%20SC:400,400italic,700,700italic%7c&amp;display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7cNoto%20Serif%20SC:400,400italic,700,700italic%7c&amp;display=swap" media=print onload='this.media="all"'><link rel=preload href=//at.alicdn.com/t/c/font_4552607_tq6stt6tcg.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=/css/loader.min.2ad0e9bbffb534e893c0ecefc44787a277cf851387e8ad9dccfbc3a5f0886dbe.css><meta property="og:type" content="website"><meta property="og:title" content="2025XJTU AI系统复习 | Bertsin"><meta property="og:description" content="本课程的中文名称设定为人工智能系统，主要讲解支持人工智能的计算机系统设计，对应的英文课程名称为 System for AI。本课程中将交替使用以下词汇：人工智能系统，AI-System 和 System for AI。本课程为微软人工智能教育与共建社区中规划的人工智能相关教程之一，在基础教程模块下，课程编号和名称为 A6-人工智能系统。"><meta property="og:url" content="https://user-xixiboliya.github.io/post/ai_system%E5%A4%8D%E4%B9%A0/"><meta property="og:site_name" content="Welcome to Bertsin Homepage!"><meta property="og:image" content="/"><meta property="article:author" content="Bertsin"><meta property="article:published_time" content="2025-05-01T15:00:00-07:10"><meta property="article:modified_time" content="2025-05-02T16:00:00-07:20"><meta name=twitter:card content="summary"><meta name=twitter:image content="/"><link rel="shortcut icon" href=/favicon.ico><link rel=stylesheet href=/css/main.min.a23591d00af1734c216ea77b270e5fab60a4f92b6498af06e6b0dde48f7c49c0.css><link rel=preload as=style href=https://npm.webcache.cn/photoswipe@5.4.4/dist/photoswipe.css onload='this.onload=null,this.rel="stylesheet"'><link rel=preload as=style href=https://npm.webcache.cn/katex@0.16.9/dist/katex.min.css onload='this.onload=null,this.rel="stylesheet"'><script src=https://npm.webcache.cn/pace-js@1.2.4/pace.min.js integrity=sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8 crossorigin=anonymous></script><link rel=stylesheet href=https://npm.webcache.cn/@reimujs/aos@0.1.0/dist/aos.css></head><body><div id=loader><div class="loading-left-bg loading-bg"></div><div class="loading-right-bg loading-bg"></div><div class=spinner-box><div class=loading-taichi><svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" shape-rendering="geometricPrecision"><path d="M303.5 432a80 80 0 01-12 160 80 80 0 0112-160z" fill="#ff5252"/><path d="M512 65a447 447 0 010 894V929a417 417 0 000-834 417 417 0 000 834v30a447 447 0 010-894zm0 30A417 417 0 01929 512 208.5 208.5.0 01720.5 720.5V592a80 80 0 000-160 80 80 0 000 160V720.5A208.5 208.5.0 01512 512 208.5 208.5.0 00303.5 303.5 208.5 208.5.0 0095 512 417 417 0 01512 95z" fill="#ff5252"/></svg></div><div class=loading-word>少女祈祷中...</div></div></div></div><script>var time=null,startLoading=()=>{time=Date.now(),document.getElementById("loader").classList.remove("loading")},endLoading=()=>{time?Date.now()-time>500?(time=null,document.body.style.overflow="auto",document.getElementById("loader").classList.add("loading")):(setTimeout(endLoading,500-(Date.now()-time)),time=null):(document.body.style.overflow="auto",document.getElementById("loader").classList.add("loading"))};window.addEventListener("DOMContentLoaded",endLoading),document.getElementById("loader").addEventListener("click",endLoading)</script><div id=copy-tooltip style="pointer-events:none;opacity:0;transition:all .2s ease;position:fixed;top:50%;left:50%;z-index:999;transform:translate(-50%,-50%);color:#fff;background:rgba(0,0,0,.5);padding:10px 15px;border-radius:10px"></div><div id=container><div id=wrap><div id=header-nav><nav id=main-nav><span class=main-nav-link-wrap><div class='main-nav-icon icon rotate'>&#xe62b;</div><a class=main-nav-link href=/>Home</a>
</span><span class=main-nav-link-wrap><div class='main-nav-icon icon'>&#xe633;</div><a class=main-nav-link href=/archives>Archives</a>
</span><span class=main-nav-link-wrap><div class='main-nav-icon icon'>&#xe63d;</div><a class=main-nav-link href=/about>About</a>
</span><span class=main-nav-link-wrap><div class='main-nav-icon icon'>&#xe639;</div><a class=main-nav-link href=/friend>Friend</a>
</span><a id=main-nav-toggle class=nav-icon></a></nav><nav id=sub-nav><a id=nav-search-btn class="nav-icon popup-trigger" title=Search></a></nav></div><header id=header><picture></picture>
<img fetchpriority=high src=/images/liyue-default.png alt="2025XJTU AI系统复习"><div id=header-outer><div id=header-title><a href=/ id=logo><h1 data-aos=slide-up>2025XJTU AI系统复习</h1></a><h2 id=subtitle-wrap data-aos=slide-down></h2></div></div></header><div id=content class=sidebar-right><aside id=sidebar><div class="sidebar-wrapper wrap-sticky"><div class=sidebar-wrap data-aos=fade-up><div class=sidebar-toc-sidebar><div class=sidebar-toc><h3 class=toc-title>Contents</h3><div class="sidebar-toc-wrapper toc-div-class"><nav id=TableOfContents><ul><li><a href=#早期的深度学习框架>早期的深度学习框架</a><ul><li><a href=#基于数据流图dag的计算框架>基于数据流图DAG的计算框架</a></li><li><a href=#自动求导>自动求导</a></li><li><a href=#图优化>图优化</a></li></ul></li></ul><ul><li><ul><li><a href=#池化操作>池化操作</a></li><li><a href=#pytorch示例代码>Pytorch示例代码</a></li><li><a href=#inception-module>Inception Module</a></li></ul></li><li><a href=#加载torchvision的自带数据集>加载torchvision的自带数据集</a></li><li><a href=#定制数据集>定制数据集</a><ul><li><a href=#生成定制的dataset>生成定制的dataset</a></li><li><a href=#生成定制的transform>生成定制的transform</a></li></ul></li><li><a href=#linux服务器和vim>linux、服务器和vim</a><ul><li><a href=#服务器框架与环境配置>服务器框架与环境配置</a></li></ul></li></ul><ul><li><a href=#卷积层映射到矩阵运算>卷积层映射到矩阵运算</a></li><li><a href=#cpu体系结构>CPU体系结构</a></li><li><a href=#并行处理硬件架构>并行处理硬件架构</a><ul><li><a href=#sisd单指令流单数据流>SISD单指令流单数据流</a></li><li><a href=#simd单指令流多数据流single-instructionmultiple-data>SIMD单指令流多数据流(Single Instruction,Multiple Data)</a></li></ul></li><li><a href=#矩阵乘的优化方案>矩阵乘的优化方案</a></li><li><a href=#gpu体系结构>GPU体系结构</a><ul><li><a href=#gpu执行模型>GPU执行模型</a></li><li><a href=#gpu内存架构>GPU内存架构</a></li><li><a href=#矩阵运算专用芯片asics>矩阵运算专用芯片ASICs</a></li><li><a href=#节省访存的核心脉动阵列>节省访存的核心：脉动阵列</a></li></ul></li></ul><ul><li><a href=#一般编译器>一般编译器</a></li><li><a href=#ai编译器的基本构成>AI编译器的基本构成</a></li><li><a href=#gcc与llvm>GCC与LLVM</a></li><li><a href=#神经网络编译器>神经网络编译器</a><ul><li><a href=#中间表示计算图>中间表示——计算图</a></li><li><a href=#前端优化>前端优化</a></li></ul></li><li><a href=#后端优化>后端优化</a></li></ul><ul><li><a href=#权重稀疏>权重稀疏</a><ul><li><a href=#非结构化剪枝>非结构化剪枝</a></li><li><a href=#结构化剪枝>结构化剪枝</a></li><li><a href=#正则化>正则化</a></li></ul></li><li><a href=#激活稀疏>激活稀疏</a></li><li><a href=#梯度稀疏>梯度稀疏</a></li><li><a href=#量化低比特计算>量化/低比特计算</a></li><li><a href=#三值网络二值网络>三值网络/二值网络</a></li><li><a href=#轻量化网络>轻量化网络</a><ul><li><a href=#模型复杂度分析>模型复杂度分析</a></li></ul></li></ul><ul><li><a href=#算子内并行>算子内并行</a></li><li><a href=#算子间并行>算子间并行</a></li><li><a href=#模型并行>模型并行</a><ul><li><a href=#模型并行的优化措施流水线>模型并行的优化措施——流水线</a></li><li><a href=#gpipe算法>Gpipe算法</a></li><li><a href=#pipedream算法>PipeDream算法</a></li></ul></li><li><a href=#数据并行>数据并行</a><ul><li><a href=#一对多>一对多</a></li><li><a href=#多对一>多对一</a></li><li><a href=#多对多>多对多</a></li><li><a href=#allreduce-的通信原语实现算法1>Allreduce 的通信原语实现算法1</a></li><li><a href=#allreduce-的通信原语实现算法2>Allreduce 的通信原语实现算法2</a></li><li><a href=#ring-allreduce>Ring Allreduce</a></li><li><a href=#通信后端>通信后端</a></li><li><a href=#集合通信原语的应用分布式sgd算法>集合通信原语的应用：分布式SGD算法</a></li><li><a href=#单机多卡分布式sgd>单机多卡分布式SGD</a></li><li><a href=#多机分布式sgd>多机分布式SGD</a></li><li><a href=#具有同步障的dist-sgd算法>具有同步障的dist-SGD算法</a></li><li><a href=#horovod>Horovod</a></li></ul></li><li><a href=#小结>小结</a></li></ul><ul><li><a href=#docker的介绍>Docker的介绍</a></li><li><a href=#镜像与容器实践>镜像与容器实践</a></li><li><a href=#公平性调度>公平性调度</a><ul><li><a href=#异构资源的公平性调度>异构资源的公平性调度</a></li><li><a href=#拓扑亲和性>拓扑亲和性</a></li><li><a href=#hived调度算法>HiveD调度算法</a></li></ul></li><li><a href=#调度的灵活性弹性与抢占>调度的灵活性——弹性与抢占</a><ul><li><a href=#hadoop-yarn抢占的实现>Hadoop YARN抢占的实现</a></li></ul></li><li><a href=#异构计算集群管理系统>异构计算集群管理系统</a></li></ul><ul><li><a href=#示例dockerfile>示例dockerfile</a></li><li><a href=#docker部署pytorch推理程序>docker部署pytorch推理程序</a></li></ul><ul><li><a href=#对抗攻击>对抗攻击</a><ul><li><a href=#攻击方法fgsm>攻击方法FGSM</a></li><li><a href=#白盒与黑盒>白盒与黑盒</a></li></ul></li><li><a href=#通用对抗攻击>通用对抗攻击</a><ul><li><a href=#对抗补丁>对抗补丁</a></li></ul></li><li><a href=#防御>防御</a><ul><li><a href=#被动防御>被动防御</a></li><li><a href=#主动防御>主动防御</a></li><li><a href=#被动防御与主动防御的区别>被动防御与主动防御的区别</a></li></ul></li></ul></nav></div></div></div><div class="sidebar-common-sidebar hidden"><div class=sidebar-author><img data-src=/avatar/avatar.webp data-sizes=auto alt=Bertsin class=lazyload><div class=sidebar-author-name>Bertsin</div><div class=sidebar-description>祈祷中...</div></div><div class=sidebar-state><div class=sidebar-state-article><div>Posts</div><div class=sidebar-state-number>24</div></div><div class=sidebar-state-category><div>Categories</div><div class=sidebar-state-number>8</div></div><div class=sidebar-state-tag><div>Tags</div><div class=sidebar-state-number>1</div></div></div><div class=sidebar-social><div class="icon-bilibili sidebar-social-icon"><a href="https://space.bilibili.com/651491932?spm_id_from=333.1007.0.0" itemprop=url target=_blank aria-label=bilibili rel="noopener external nofollow noreferrer"></a></div><div class="icon-email sidebar-social-icon"><a href=linboxi123@163.com itemprop=url target=_blank aria-label=email rel="noopener external nofollow noreferrer"></a></div><div class="icon-github sidebar-social-icon"><a href=https://github.com/user-xixiboliya itemprop=url target=_blank aria-label=github rel="noopener external nofollow noreferrer"></a></div><div class="icon-zhihu sidebar-social-icon"><a href=https://www.zhihu.com/people/lllll-19-64-21 itemprop=url target=_blank aria-label=zhihu rel="noopener external nofollow noreferrer"></a></div></div><div class=sidebar-menu><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/ aria-label=Home></a><div class='sidebar-menu-icon icon rotate'>&#xe62b;</div><div class=sidebar-menu-link>Home</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/archives aria-label=Archives></a><div class='sidebar-menu-icon icon'>&#xe633;</div><div class=sidebar-menu-link>Archives</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/about aria-label=About></a><div class='sidebar-menu-icon icon'>&#xe63d;</div><div class=sidebar-menu-link>About</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/friend aria-label=Friend></a><div class='sidebar-menu-icon icon'>&#xe639;</div><div class=sidebar-menu-link>Friend</div></div></div></div><div class=sidebar-btn-wrapper style=position:static><div class="sidebar-toc-btn current"></div><div class=sidebar-common-btn></div></div></div></div><div class=sidebar-widget></div></aside><section id=main><article class="h-entry article" itemprop=blogPost itemscope itemtype=https://schema.org/BlogPosting><div class=article-inner data-aos=fade-up><div class=article-meta><div class=article-date><span class=article-date-link data-aos=zoom-in><time datetime="2025-05-01 15:00:00 -0710 -0710" itemprop=datePublished>2025-05-01</time>
<time style=display:none id=post-update-time>2025-05-02</time></span></div><div class=article-category><a class=article-category-link href=/categories/technology data-aos=zoom-in>TECHNOLOGY</a></div></div><div class=hr-line></div><div class="e-content article-entry" itemprop=articleBody><h1 id=深度神经网络基础><a class=header-anchor href=#%e6%b7%b1%e5%ba%a6%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80></a>深度神经网络基础</h1><h2 id=早期的深度学习框架><a class=header-anchor href=#%e6%97%a9%e6%9c%9f%e7%9a%84%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%a1%86%e6%9e%b6></a>早期的深度学习框架</h2><p>主要用来CNN网络图像识别，由一些常用的layers组成。
主要特点就是通过.yaml这样的配置文件定义神经网络，可由一些常用的layer构成一个简单的图，支持多设备加速，代表框架是由UCB提出的Caffe。</p><h3 id=基于数据流图dag的计算框架><a class=header-anchor href=#%e5%9f%ba%e4%ba%8e%e6%95%b0%e6%8d%ae%e6%b5%81%e5%9b%bedag%e7%9a%84%e8%ae%a1%e7%ae%97%e6%a1%86%e6%9e%b6></a>基于数据流图DAG的计算框架</h3><ul><li>基本数据结构是tensor</li><li>基本运算单元接收N个输入tensor，输出M个tensor。 tensorflow中有大于400个基本的operator
在数据流图中，<strong>节点</strong>表示<code>operator</code>，<strong>边</strong>表示<code>tensor</code>。有一些特殊的节点用来构建控制流，控制边用来表示节点之间的依赖关系。</li></ul><h4 id=静态数据流图tensorflow><a class=header-anchor href=#%e9%9d%99%e6%80%81%e6%95%b0%e6%8d%ae%e6%b5%81%e5%9b%betensorflow></a>静态数据流图TensorFlow</h4><p>先定义后执行，静态图表示在编译时首先生成神经网络的结构，然后再执行对应的操作。</p><ul><li>静态图定义图后可以全局图优化</li></ul><h4 id=动态数据流图pytorch><a class=header-anchor href=#%e5%8a%a8%e6%80%81%e6%95%b0%e6%8d%ae%e6%b5%81%e5%9b%bepytorch></a>动态数据流图pytorch</h4><p>边定义边运行，每次编译时都需要构建一个新的计算图。</p><p><strong>计算流图</strong>：引入中间变量将复杂的函数分解成一系列基本函数，构成计算流图。</p><h3 id=自动求导><a class=header-anchor href=#%e8%87%aa%e5%8a%a8%e6%b1%82%e5%af%bc></a>自动求导</h3><p>前向计算并且保留计算结果，有个主要问题是需要保存大量的计算结果。在导数的计算也可以表示成数据流图。方便全局图优化与节省内存。</p><p>现代大部分框架同时提供：</p><ul><li>自动求导系统，针对常见函数的组合</li><li>大量内置的Operators和与之对应的反向函数</li><li>为用户提供接口，自己提供反向函数。</li></ul><h3 id=图优化><a class=header-anchor href=#%e5%9b%be%e4%bc%98%e5%8c%96></a>图优化</h3><p>先定义后执行的模式允许框架在计算前看到全图信息。数据流图作为深度学习框架中的高层中间表示，可以允许任何等价图优化Pass去化简计算流图或提高执行效率。</p><h4 id=图优化demm自动融合><a class=header-anchor href=#%e5%9b%be%e4%bc%98%e5%8c%96demm%e8%87%aa%e5%8a%a8%e8%9e%8d%e5%90%88></a>图优化：DEMM自动融合</h4><p>参考文章：<a href=https://zhuanlan.zhihu.com/p/435908830>深入浅出GPU优化系列：GEMM优化（一） - 知乎</a></p><h4 id=计算内核与多硬件支持><a class=header-anchor href=#%e8%ae%a1%e7%ae%97%e5%86%85%e6%a0%b8%e4%b8%8e%e5%a4%9a%e7%a1%ac%e4%bb%b6%e6%94%af%e6%8c%81></a>计算内核与多硬件支持</h4><p><code>kernel</code>定义了一个算子在某种具体设备的计算实现。</p><p>每个operator都可以注册多个kernel，取决于数据类型、计算设备不同。runtime框架会自动根据operator的设备类型，数据类型和属性选择对应的kernel来执行。</p><p>一段反向传播代码：
$\sum({x}*{y})+{z} = c$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>N,D <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(N,D)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(N,D)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(N,D)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> y
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> a <span style=color:#f92672>+</span> z
</span></span><span style=display:flex><span>c <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(b)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>grad_c <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>  
</span></span><span style=display:flex><span>grad_b <span style=color:#f92672>=</span> grad_c <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>ones((N,D))
</span></span><span style=display:flex><span>grad_a <span style=color:#f92672>=</span> grad_b<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>grad_z <span style=color:#f92672>=</span> grad_b<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>grad_x <span style=color:#f92672>=</span> grad_a <span style=color:#f92672>*</span> y
</span></span><span style=display:flex><span>grad_y <span style=color:#f92672>=</span> grad_a <span style=color:#f92672>*</span> x
</span></span></code></pre></div><p>这里给出$grad_{x}$和$grad_{y}$的求解:
<img src=output_image/e54e60ea41b6c5166c8077b3778e51a1.png alt></p><h1 id=深度学习框架pytorch><a class=header-anchor href=#%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%a1%86%e6%9e%b6pytorch></a>深度学习框架pytorch</h1><h3 id=池化操作><a class=header-anchor href=#%e6%b1%a0%e5%8c%96%e6%93%8d%e4%bd%9c></a>池化操作</h3><p><img src=output_image/b5b3ce621009e04b0a6bbe03a966bb5a.png alt></p><h3 id=pytorch示例代码><a class=header-anchor href=#pytorch%e7%a4%ba%e4%be%8b%e4%bb%a3%e7%a0%81></a>Pytorch示例代码</h3><p>以下代码实现了CNN：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Net</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>		super(Net,self)<span style=color:#f92672>.</span>__init__()   <span style=color:#75715e># 请你填空</span>
</span></span><span style=display:flex><span>		self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>10</span>,kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>) <span style=color:#75715e># [26,26](inchannel,outchannel,kernelsize)</span>
</span></span><span style=display:flex><span> 		self<span style=color:#f92672>.</span>conv2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>20</span>,kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>) <span style=color:#75715e># [22,22]</span>
</span></span><span style=display:flex><span> 		self<span style=color:#f92672>.</span>mp <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MaxPool2d(<span style=color:#ae81ff>2</span>) <span style=color:#75715e># 不改变通道数 [11,11]</span>
</span></span><span style=display:flex><span> 		self<span style=color:#f92672>.</span>fc <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#960050;background-color:#1e0010>?</span>,<span style=color:#ae81ff>10</span>)  <span style=color:#75715e># </span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self,x):
</span></span><span style=display:flex><span>		in_size <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>		x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(F<span style=color:#f92672>.</span>mp(self<span style=color:#f92672>.</span>conv1(x))) <span style=color:#75715e># 10 ，12 ，12</span>
</span></span><span style=display:flex><span>		x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(F<span style=color:#f92672>.</span>mp(self<span style=color:#f92672>.</span>conv2(x))) <span style=color:#75715e># 20，4，4</span>
</span></span><span style=display:flex><span>		x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>view(in_size,<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>		x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc(x)  
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> F<span style=color:#f92672>.</span>log_softmax(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>)  <span style=color:#75715e># 示例输入</span>
</span></span></code></pre></div><p><code>?</code>处应为$20 \times 4 \times4 = 320$。</p>$$N =\frac{Width+2\cdot Padding - Kernelsize}{stride} + 1$$<p>定义网络时，需要继承<code>nn.Module</code>并且实现<code>forward()</code>方法，将具有可学习参数的层放在<code>__init__</code>中。</p><h3 id=inception-module><a class=header-anchor href=#inception-module></a>Inception Module</h3><p>定义成<code>self</code>形式，越在后面的层，越外面。
<code>branch3x3dbl =self.branch3x3dbl_3(self.branch3x3dbl_2(self.branch3x3dbl_1(x)))</code>
<img src=output_image/564d58320b3f384806cb0d79f640a00d.png alt></p><p>1、采用1x1卷积核将不同通道的信息融合。使用1x1卷积核虽然参数量增加了，但是能够显著的降低计算量。
2、<code>Inception</code>模块分为不同的分支是为了提取到更多的特征，同时也可以加速训练过程。
3、<code>Inception Moudel</code>由4个分支组成，最终4个分支在<code>dim = 1(channels)</code>上进行组合。最终输出的通道总数为<code>24+16+24+24 = 88</code></p><p>代码有以下特点：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>branch3x3dbl_1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(inchannels,<span style=color:#ae81ff>10</span>,kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>branch3x3dbl_2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>24</span>,kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 在forward中：</span>
</span></span><span style=display:flex><span>	branch3x3dbl_1 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>branch3x3dbl_1(x)
</span></span><span style=display:flex><span>	output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>branch3x3dbl_2(branch3x3dbl_1)
</span></span><span style=display:flex><span><span style=color:#75715e># 这样套起来，再将每个分支的channel拼起来即可output</span>
</span></span></code></pre></div><h2 id=加载torchvision的自带数据集><a class=header-anchor href=#%e5%8a%a0%e8%bd%bdtorchvision%e7%9a%84%e8%87%aa%e5%b8%a6%e6%95%b0%e6%8d%ae%e9%9b%86></a>加载torchvision的自带数据集</h2><p><code>torchvision</code>实现了常用的图像数据加载功能，例如Imagenet、CIFAR10、 MNIST等，以及常用的数据转换操作，这极大地方便了数据加载。</p><p>这里给出了如何使用自带数据集的示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>transform <span style=color:#f92672>=</span> transform<span style=color:#f92672>.</span>compose([
</span></span><span style=display:flex><span>	transform<span style=color:#f92672>.</span>ToTensor(),
</span></span><span style=display:flex><span>	transform<span style=color:#f92672>.</span>Normalize((<span style=color:#ae81ff>0.5</span>,<span style=color:#ae81ff>0.5</span>,<span style=color:#ae81ff>0.5</span>),(<span style=color:#ae81ff>0.5</span>,<span style=color:#ae81ff>0.5</span>,<span style=color:#ae81ff>0.5</span>))
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trainset <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>CFAIR10(root<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./data&#39;</span>,train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,download<span style=color:#f92672>=</span>Trued,transform<span style=color:#f92672>=</span>transform)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trainloader <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>DataLoader(trainset,batchsize<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><h2 id=定制数据集><a class=header-anchor href=#%e5%ae%9a%e5%88%b6%e6%95%b0%e6%8d%ae%e9%9b%86></a>定制数据集</h2><p>首先安装依赖库：<code>scikit-image</code>（图像输入输出和转换）、<code>pandas</code>。</p><h3 id=生成定制的dataset><a class=header-anchor href=#%e7%94%9f%e6%88%90%e5%ae%9a%e5%88%b6%e7%9a%84dataset></a>生成定制的dataset</h3><p>代码示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> skimage <span style=color:#f92672>import</span> io,transform
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> Dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MyDataset</span>(Dataset):
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>def</span> __init__(self,csv_file,root_dir,transform<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>		self<span style=color:#f92672>.</span>root_dir <span style=color:#f92672>=</span> root_dir
</span></span><span style=display:flex><span>		self<span style=color:#f92672>.</span>transform <span style=color:#f92672>=</span> transform
</span></span><span style=display:flex><span>		self<span style=color:#f92672>.</span>frame <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read(csv_file)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>def</span> __len__(self):
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>frame)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>def</span> __getitem__(self,idx):
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>is_tensor(idx):
</span></span><span style=display:flex><span>			idx <span style=color:#f92672>=</span> idx<span style=color:#f92672>.</span>tolist()
</span></span><span style=display:flex><span>			
</span></span></code></pre></div><p>需要重写<code>__len__ __getitem__ __init__</code>方法：</p><ul><li><code>__getitem__</code>用来支持dataset索引，这样<code>dataset[i]</code>可以用来得到第i个样本</li><li><code>__len__</code>让<code>len(dataset)</code>返回<code>dataset</code>的大小</li></ul><h3 id=生成定制的transform><a class=header-anchor href=#%e7%94%9f%e6%88%90%e5%ae%9a%e5%88%b6%e7%9a%84transform></a>生成定制的transform</h3><p>部分神经网络需要固定大小的图像输出，需要对图像进行一些预处理。可以创建三种<code>transform</code>例如<code>Rescale Randomcrop ToTensor</code>，将这三个写成可调用的类。<strong>需要重写__call__ 和__init__方法。</strong></p><h2 id=linux服务器和vim><a class=header-anchor href=#linux%e6%9c%8d%e5%8a%a1%e5%99%a8%e5%92%8cvim></a>linux、服务器和vim</h2><p><img src=output_image/d57c91ec735e42a9ed2b66c28c4f65b1.png alt></p><p>To Name But A Few,上述键位包含了<a href=https://zhuanlan.zhihu.com/p/122215250>宏录制</a>、<code>>></code>行首缩进、<code>S</code>删除行并且直接进入插入模式、<code>s</code>删除字符并插入、<code>Y</code>拷贝行、<code>f</code>行内字符查找(按下<code>;</code>继续查找)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 例如查找对应行，将匹配的字符替换成bar</span>
</span></span><span style=display:flex><span>:<span style=color:#f92672>{</span>作用范围<span style=color:#f92672>}</span>s/target<span style=color:#ae81ff>\c</span>/bar/g 
</span></span><span style=display:flex><span><span style=color:#75715e># g 是global的意思 \c是大小写不敏感的替换 </span>
</span></span></code></pre></div><h3 id=服务器框架与环境配置><a class=header-anchor href=#%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%a1%86%e6%9e%b6%e4%b8%8e%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae></a>服务器框架与环境配置</h3><p><strong>Cuda</strong>，是一种由nvidia退出的通用并行计算架构，包含了CUDA指令集架构ISA以及GPU内部的并行计算引擎。
<strong>Cudnn</strong>，是用于深度神经网络的GPU加速库，强调性能、易用性和低内存开销。Nvidia cudnn可以集成到更高级别的机器学习框架中，如tensorflow、pytorch。</p><p><strong>Tensorboard</strong>，是一组用于<a href="https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96&amp;spm=1001.2101.3001.7020">数据可视化</a>的工具。它包含在流行的开源机器学习库 Tensorflow中。主要功能包括：</p><ol><li>可视化模型的网络架构</li><li>跟踪模型指标，如损失和准确性等</li><li>检查机器学习工作流程中权重、偏差和其他组件的直方图</li><li>显示非表格数据，包括图像、文本和音频</li><li>将高维嵌入投影到低维空间
使用说明：</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 在开头: </span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.tensorboard <span style=color:#f92672>import</span> SummaryWriter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>writer <span style=color:#f92672>=</span> SummaryWriter(<span style=color:#e6db74>&#39;runs/exp&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 接着在每个epoch循环内加上：</span>
</span></span><span style=display:flex><span>writer<span style=color:#f92672>.</span>add_scalar(<span style=color:#e6db74>&#39;Loss/train&#39;</span>,loss,epoch)
</span></span><span style=display:flex><span>writer<span style=color:#f92672>.</span>add_scalar(<span style=color:#e6db74>&#39;Accuracy/train&#39;</span>,accuracy,epoch)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 最后关闭writer</span>
</span></span><span style=display:flex><span>writer<span style=color:#f92672>.</span>close()
</span></span></code></pre></div><p><code>remote server</code>开启<code>tensorboard</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tensorboard --logdir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./runs/exp1&#39;</span>
</span></span><span style=display:flex><span>ssh -L 7707:localhost:6006 root277@ip
</span></span></code></pre></div><p>其中,7707是本地机器的端口，<code>localhost</code>是远程SSH服务器的6006端口。</p><p>在客户端浏览器中<code>https://localhost:7707</code>即可访问<code>tensorboard</code>。</p><h1 id=矩阵运算与计算机体系结构><a class=header-anchor href=#%e7%9f%a9%e9%98%b5%e8%bf%90%e7%ae%97%e4%b8%8e%e8%ae%a1%e7%ae%97%e6%9c%ba%e4%bd%93%e7%b3%bb%e7%bb%93%e6%9e%84></a>矩阵运算与计算机体系结构</h1><h2 id=卷积层映射到矩阵运算><a class=header-anchor href=#%e5%8d%b7%e7%a7%af%e5%b1%82%e6%98%a0%e5%b0%84%e5%88%b0%e7%9f%a9%e9%98%b5%e8%bf%90%e7%ae%97></a>卷积层映射到矩阵运算</h2><p>卷积也是一种线性运算：
<img src=output_image/8b7edf13019a89abd12c8a751e788b0a.png alt></p><h2 id=cpu体系结构><a class=header-anchor href=#cpu%e4%bd%93%e7%b3%bb%e7%bb%93%e6%9e%84></a>CPU体系结构</h2><p><img src=output_image/ec30b59b3824b5bdd69c1cd347eb4ad5.png alt>对于<strong>CPU</strong>而言，L1 cache集中在<strong>CPU</strong> 内部，<strong>Cache</strong>主要由<strong>SRAM</strong>组成，位于<strong>CPU</strong>和主存储器<strong>DRAM</strong>(内存)之间。采用较深的内存架构，在调度上采用访存预取和各种较为成熟的预取预测机制。</p><h2 id=并行处理硬件架构><a class=header-anchor href=#%e5%b9%b6%e8%a1%8c%e5%a4%84%e7%90%86%e7%a1%ac%e4%bb%b6%e6%9e%b6%e6%9e%84></a>并行处理硬件架构</h2><h3 id=sisd单指令流单数据流><a class=header-anchor href=#sisd%e5%8d%95%e6%8c%87%e4%bb%a4%e6%b5%81%e5%8d%95%e6%95%b0%e6%8d%ae%e6%b5%81></a>SISD单指令流单数据流</h3><p>每个指令部件每次仅译码一条指令，而且在执行时仅为操作部件提供一份数据；串行计算，在时钟周期内，CPU只能处理一个数据流。</p><h3 id=simd单指令流多数据流single-instructionmultiple-data><a class=header-anchor href=#simd%e5%8d%95%e6%8c%87%e4%bb%a4%e6%b5%81%e5%a4%9a%e6%95%b0%e6%8d%ae%e6%b5%81single-instructionmultiple-data></a>SIMD单指令流多数据流(Single Instruction,Multiple Data)</h3><p>目前 CPU 和 GPU 主要用到的并行计算架构是 SIMD，处理器硬件中添加了多个 PU（Process Unit）单元，此时一个控制器控制多个处理器，同时对一组数据中每一个数据分别执行相同的操作，实现并行计算。</p><p>SIMD仍然是单线程，硬件上仅需要一个计算核心，主要执行向量矩阵等数组运算。处理单元数量很多。</p><h2 id=矩阵乘的优化方案><a class=header-anchor href=#%e7%9f%a9%e9%98%b5%e4%b9%98%e7%9a%84%e4%bc%98%e5%8c%96%e6%96%b9%e6%a1%88></a>矩阵乘的优化方案</h2><p><img src=output_image/2086aa12a08f0d9818fa33c454e24d2c.png alt>
优化方案：对图示中A的行的每个数据进行广播，广播次数为B中的行（在图中是4）。设$A_{m\times n}$，中的行锚定了$C$中的行，$C$中的每一行进行了$m$（$m-1$）次累加。
<img src=output_image/3b110523325a9830d27ed2d6a7a9759f.png alt>
当然，$A$不仅仅可以$load$一行。
<img src=output_image/639d344daeb8e75822327337d7b7c962.png alt></p><h2 id=gpu体系结构><a class=header-anchor href=#gpu%e4%bd%93%e7%b3%bb%e7%bb%93%e6%9e%84></a>GPU体系结构</h2><p>对于向量化的计算算子${SSE}、{AVX}$，执行计算的ALU有许多。GPU内部具有许多ALU。</p><p>GPU由上千个简单的核心组成，每个core的结构相对简单，不支持分支预测、推测执行、乱序执行等。</p><h3 id=gpu执行模型><a class=header-anchor href=#gpu%e6%89%a7%e8%a1%8c%e6%a8%a1%e5%9e%8b></a>GPU执行模型</h3><h4 id=simt><a class=header-anchor href=#simt></a>SIMT</h4><p>将一组Cores组织成一个cluster，在同一时间这些cores都执行相同的指令，32个线程为一组构成warp，以warp为单位调度到cores上。一个warp内所有线程执行相同指令，但操作在不同的数据上。</p><p>在 CUDA 编程模型中，每一个线程块（thread block）内部需要有很多并行线程，隐式分成了若干个 Warp，每个 Warp 包含串行交错的访存和计算。GPU 通过 Warp Scheduler 动态交错执行，如果一组 Warp0 流水阻塞就会切到下一个 Warp1，隐式通过 Warp 的并行掩盖指令流水阻塞，因此开发者可以得到较好的性能。
<img src=output_image/2853ecbc4b0f3553073f4bec7c9b1f67.png alt>
SM（Streaming Multiprocessors）称作流式多处理器，核心组件包括 CUDA 核心、共享内存、寄存器等。SM 包含很多为线程执行数学运算的 core，是英伟达 GPU 的核心，在 CUDA 中可以执行数百个线程、一个 block 上线程放在同一个 SM 上执行，一个 SM 有限的 Cache 制约了每个 block 的线程数量。</p><p>每一个$Kernel$下有$device$，下分诚不同的grid，每个grid下有许多block，每个block下的基本单元是线程，32个线程被隐式地划分为一组的warp（执行单元），通过warp的交错并行计算掩盖指令的流水线阻塞。运行时每个scheduler选择一个warp，发送给CUDA执行。(dispatches)</p><h3 id=gpu内存架构><a class=header-anchor href=#gpu%e5%86%85%e5%ad%98%e6%9e%b6%e6%9e%84></a>GPU内存架构</h3><p>不同于一般$CPU$的$cache$，L1和L2cache都在Core中，GPU的L1 cache在SM中，L2在SM外。
<img src=output_image/9f3b60665c7c1b5c056cfdb146420cf7.png alt></p><p>如何在GPU上高效地计算矩阵乘法，应该提高访存计算比，从$global$尽可能多的复用数据。
<img src=output_image/49d8ad32a2986666b7d5869713336d9b.png alt>
在图中，矩阵对应块的相乘要求BlockitemsK的索引要一致。Tile A 和B 从memory中load到shared memory中，该SM中的所有$warps$都可以读取A和B。
<img src=output_image/3dd72b05c398a29b33e3342c07f22a26.png alt></p><p><img src=output_image/eab12f762afa5317fc980a3a3e51f6a2.png alt>
<img src=output_image/60d3c3718b5737847baa8d88b9013471.png alt>
大量使用寄存器使得并发运行的block较少，访存计算比较低，采用软件流水线的方法隐藏访存延时。</p><h3 id=矩阵运算专用芯片asics><a class=header-anchor href=#%e7%9f%a9%e9%98%b5%e8%bf%90%e7%ae%97%e4%b8%93%e7%94%a8%e8%8a%af%e7%89%87asics></a>矩阵运算专用芯片ASICs</h3><ul><li>Tensor Processing Unit(TPU) 一代TPU采用8bit整型作为计算数据类型。TPU 计算方式是matrix、 vector CPU with AVX/SSE GPU 用 vector 计算。</li><li>精简指令集MXU 乘加单元 可以在一个时间周期计算数十万个计算</li><li>UB 24MB of SRAM</li><li>Activation Unit 硬件激活单元</li></ul><h3 id=节省访存的核心脉动阵列><a class=header-anchor href=#%e8%8a%82%e7%9c%81%e8%ae%bf%e5%ad%98%e7%9a%84%e6%a0%b8%e5%bf%83%e8%84%89%e5%8a%a8%e9%98%b5%e5%88%97></a>节省访存的核心：脉动阵列</h3><p>CPUs和GPUs通常需要花费大量的功耗去读取寄存器的值，脉动阵列的设计思想是将多个ALU运算单元串起来，从而避免每次计算都读取寄存器，缺点是要求计算符合特点的规则。</p><p>下面是<strong>计算图</strong>：
<img src=output_image/a82f2b4c6f7304d52e4771a05a0d0d0a.png alt>
<img src=output_image/bc28b04bc747f21824efd313a2639578.png alt>
<img src=output_image/16932156f87d4a6b86e24c2f9c9f0783.png alt>
<img src=output_image/aec428a03ee531c8f453265b76fcf2fa.png alt>
上图归结起来可以理解为：在$w_{i1} w_{{i2}}$的方向传播$x_{i}$，横向传播每一次$w$与$x$的乘积。</p><h1 id=ai编译器与编译优化><a class=header-anchor href=#ai%e7%bc%96%e8%af%91%e5%99%a8%e4%b8%8e%e7%bc%96%e8%af%91%e4%bc%98%e5%8c%96></a>AI编译器与编译优化</h1><h2 id=一般编译器><a class=header-anchor href=#%e4%b8%80%e8%88%ac%e7%bc%96%e8%af%91%e5%99%a8></a>一般编译器</h2><p>笔者认为有必要了解一般编译器的大致逻辑，以便更好地使用<code>gcc -E example.c -o example.i</code>这样的生成中间代码的命令。
<img src=output_image/87ffe759da19b091fe2ddcb116807800.png alt>
需要指明的是，<code>.c</code>是源代码文件，<code>.i</code>是预处理后的源代码，包含展开的宏和包含的头文件，可以通过参数<code>-E</code>选项生成此文件。<code>.s</code>是汇编代码，包含低级汇编语言指令，例如<code>gcc -S example.c -o example.s</code>。<code>gcc -c example.c -o example.o</code>是生成目标文件。<code>-o</code> 选项在 <code>GCC</code> 中用于指定输出文件的名称。</p><h2 id=ai编译器的基本构成><a class=header-anchor href=#ai%e7%bc%96%e8%af%91%e5%99%a8%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%9e%84%e6%88%90></a>AI编译器的基本构成</h2><p><img src=output_image/6b2a500e72d36a4865bbceceecded5f6.png alt></p><ul><li><code>front-end</code>：主要负责词法和语法分析，检查源程序语法和语义，然后将源代码转化为抽象语法树。</li><li><code>back-end</code>：将已经优化的中间代码转化为不同平台的编译器代码。</li></ul><h2 id=gcc与llvm><a class=header-anchor href=#gcc%e4%b8%8ellvm></a>GCC与LLVM</h2><p><img src=output_image/6d8936eb3f51194ada474318c3aa9f98.png alt></p><p><code>Pass：</code>对源程序的一次完整扫描与优化处理。<code>IR</code>有<code>Graph IR</code>。</p><pre tabindex=0><code>专用加速芯片爆发导致性能可移植性成为一种刚需。不同厂商提供 XPU 的 ISA（Instruction Set Architecture） 多种多样， 一般缺乏如 GCC、 LLVM 等编译工具链，使得针对 CPU 和 GPU已有的优化算子库和针对语言的优化 Pass 很难短期移植到NPU 上，难以复用。
</code></pre><p><strong>IR的表达层级差异</strong>：AI 编译器的<code>IR</code>表达层级较高，用来抽象描述深度学习模型中的运算<code>Convlusion</code>等，甚至部分会有<code>Transformer</code>带有图的结构。</p><p><strong>IR的优化策略差异</strong>：可以在<code>high-level</code>的<code>IR</code>上面做<code>operation fusion</code>。AI编译器也可以降低计算精度。</p><h2 id=神经网络编译器><a class=header-anchor href=#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%bc%96%e8%af%91%e5%99%a8></a>神经网络编译器</h2><p>前端一般是<code>dpytorch</code>，<code>tensorflow</code>这样的语言，后端为神经网络加速器，如<code>GPU TPU FPGA</code>这样的。中间表达为计算图 算子表达式 ，如<code>DAG TVM IR</code>。</p><h3 id=中间表示计算图><a class=header-anchor href=#%e4%b8%ad%e9%97%b4%e8%a1%a8%e7%a4%ba%e8%ae%a1%e7%ae%97%e5%9b%be></a>中间表示——计算图</h3><p>基本数据结构是Tensor，基本运算单元是Operator，由最基本的代数算子组成，例如batchnorm conv这样的。每一个operator接收N个tensor，并且输出M个tensor。tensorflow中有大于400个的基本的`operator。</p><h3 id=前端优化><a class=header-anchor href=#%e5%89%8d%e7%ab%af%e4%bc%98%e5%8c%96></a>前端优化</h3><p>通过图的等价变化化简计算图，从而降低计算复杂度或者内存开销。允许任何等价图优化<code>Pass</code>去化简计算流图或提高执行效率。</p><p>前端优化与<strong>硬件无关</strong>，意味着可以将计算图优化应用于各种后端目标。优化方式有：</p><ul><li>算术表达式化简 $(A\times B)\dot{D} + (A\times B)\dot{C} =(A \times B)(C+D)$</li><li>公共子表达式消除，例如：<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span>a <span style=color:#f92672>=</span> b <span style=color:#f92672>*</span> c <span style=color:#f92672>+</span> g
</span></span><span style=display:flex><span>d <span style=color:#f92672>=</span> b <span style=color:#f92672>*</span> c <span style=color:#f92672>+</span> g
</span></span><span style=display:flex><span><span style=color:#75715e>//可以转化为
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>temp <span style=color:#f92672>=</span> b <span style=color:#f92672>*</span> c
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> temp <span style=color:#f92672>+</span> g
</span></span><span style=display:flex><span>d <span style=color:#f92672>=</span> temp <span style=color:#f92672>+</span> e
</span></span></code></pre></div>如下图所示，发现<code>sub 6</code>和<code>sub 10</code>等等重复，因此可以进行公共表达式消除。
<img src=output_image/de0457c8da15447098633d820de37653.png alt></li><li>死代码消除
死代码消除一般不是在定义神经网络模型结构时候引起的，而是其他图优化Pass造成的结果，因此死代码消除Pass常常在其他图优化pass后被应用。如无用的控制流、推理时仅删除训练相关的子图。</li><li>常量折叠<ul><li>例如<code>i = 320 * 20</code>，不需要<code>%a:load 320,%b:load 20,%x:mul %a %b</code>，而是可以直接<code>load 6400</code> 。</li><li><code>BN</code>折叠，主要用于将 ​<strong>​Batch Normalization（BN）层​</strong>​ 的参数合并到其前驱的 ​<strong>​卷积层（Conv）​</strong>​ 或 ​<strong>​全连接层（FC）​</strong>​ 中，从而减少推理时的计算量并提升部署效率。其将<code>Batch Normailzation</code>各层输入进行归一化。对于${x_{1},x_{2},\dots,x_{m}}$，在$batch$维度拼接。训练时，$\gamma \beta \mu \sigma$一直在更新，推理时则是固定，在推理过程中，BN像是对上一层结果进行的简单线性变换。由于卷积也是一个线性变换，这两个操作甚至可以合并成一个单一的线性变换，这将提升推理速度。$$\frac{1}{m}\sum\limits_m {{x_m}} = \mu \quad \frac{1}{m} {\sum\limits_{i = 0}^m {{{({x_i} - \mu )}^2}} } = {\sigma ^2}\quad {\hat x_i} = \frac{{{x_i} - \mu }}{{\sqrt {{\sigma ^2} + \varepsilon } }} \quad{y_i} \leftarrow \gamma {\hat x_i} + \beta $$</li><li><code>BN</code>折叠，合并BN层后的卷积层权重和偏置可以表示为$z = {W_{fold}} \times x + {b_{fold}}$，减少了访存时间。其中：$${W_{fold}} = \gamma \frac{w}{{\sqrt {{\sigma ^2} + \varepsilon } }} + \beta \quad {b_{fold}} = \gamma \frac{{b - \mu }}{{\sqrt {{\sigma ^2} + \varepsilon } }} + \beta $$</li></ul></li><li>算子融合，向量化的多个算子的操作可以合并成一个向量化操作。既可以减少内核启动开销，又可以减少内存的读取，提高计算精密度</li><li>GEMM自动融合。将输入张量合并成一个大张量来实现相同的算子合并成更大的算子，提高并行度。</li><li>布局转换。讨论了<code>tensor</code>的布局方式，有列优先存储、行优先存储这样的。在深度学习领域，多维数据通过多维数组存储，例如卷积神经网络通常采用四维数组保存，即<code>N H W C</code>。<code>N</code>是<code>batch</code>数量。<ul><li>对于不同的排列方式，有$NCHW$(pytorch的存储结构) 和$NHWC$ ，如果需要让你写出这两种数据布局的划分，可以这样看：对于NHWC而言，C被优先遍历，其次是W、H。因而在<code>1 7 13</code>这样的序列是限定遍历Channel，再按照横行遍历<code>W--width</code>。 <img src=output_image/4bb1d5d1fb19f54a3eb915e93173ee57.png alt></li></ul></li><li>内存分配：<ul><li>内存复用：利用AI编译器对计算图中的数据流进行分析，以允许重用内存。</li><li>替代操作：如果一块内存不再需要，且下一个操作是<code>element-wise</code>(操作前后tensor的维度一样)的，可以做原地操作覆盖内存，无需新内存。</li><li>内存共享：两个数据共享同一块内存空间，并且其中有一个数据参与计算后不再需要，后一个数据可以覆盖前一个数据。这于内存共享的区别在于不要求<code>element-wise</code>，只要求前一个数据的大小足够容纳后一个数据的存放。下图中红颜色代表了内存共享。 <img src=output_image/a2180b5ef289dee90e5d9fa6022bbc4e.png alt></li></ul></li></ul><blockquote><p>请简要简述一下AI编译器前端优化中内存共享与替代操作的区别。</p></blockquote><h2 id=后端优化><a class=header-anchor href=#%e5%90%8e%e7%ab%af%e4%bc%98%e5%8c%96></a>后端优化</h2><ul><li>循环展开，这个不必多说，尽量减少<code>for</code>循环的次数。</li><li>循环分块，这是将数据分块存储再cache中，从而使处理器在每次循环迭代中尽可能直接访问缓存中的数据块，避免需要从贮存中读取数据带来的访存模式。<img src=output_image/5740b0cc7b167244855f0638c4d0c2b3.png alt></li><li>循环融合。将相邻或紧密间隔的循环融合在一起<code>for(int i=0;i&lt;N;i++)</code>和<code>for(int i=0;i&lt;N-1;i++)</code>这样相近的循环融合在一起。</li><li>循环拆分。</li></ul><h1 id=神经网络的稀疏化与轻量化><a class=header-anchor href=#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e7%a8%80%e7%96%8f%e5%8c%96%e4%b8%8e%e8%bd%bb%e9%87%8f%e5%8c%96></a>神经网络的稀疏化与轻量化</h1><h2 id=权重稀疏><a class=header-anchor href=#%e6%9d%83%e9%87%8d%e7%a8%80%e7%96%8f></a>权重稀疏</h2><p>如果小于阈值则置零。
<img src=output_image/cc68ee4a9e47886c19d870bb15124688.png alt></p><h3 id=非结构化剪枝><a class=header-anchor href=#%e9%9d%9e%e7%bb%93%e6%9e%84%e5%8c%96%e5%89%aa%e6%9e%9d></a>非结构化剪枝</h3><p>直接减除低于阈值的权值，神经元数量未发生变化，这是一种细粒度的剪枝。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.utils.prune <span style=color:#66d9ef>as</span> prune
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> LeNet()<span style=color:#f92672>.</span>to(device<span style=color:#f92672>=</span>device)
</span></span><span style=display:flex><span>module <span style=color:#f92672>=</span> nodel<span style=color:#f92672>.</span>convl
</span></span><span style=display:flex><span>print(list(module<span style=color:#f92672>.</span>named_parameters())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>prune<span style=color:#f92672>.</span>random_unstructured(module,name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;weight&#34;</span>,amount<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>)
</span></span><span style=display:flex><span>prune<span style=color:#f92672>.</span>l1_unstructured(module,name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;weight&#34;</span>,amount<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>) <span style=color:#75715e># 剪掉30%</span>
</span></span><span style=display:flex><span>print(list(module<span style=color:#f92672>.</span>named_buffers()))
</span></span><span style=display:flex><span>print(module<span style=color:#f92672>.</span>weight)
</span></span></code></pre></div><p>优点：稀疏化矩阵的存储，只需存储非零元素的位置，降低了存储体积，但是造成了一定的时间开销，有一些难于加速的问题；计算的收益需要依赖于特定硬件的推理库进行加速，否则会访问到空值。</p><h4 id=结构化剪枝与非结构化剪枝><a class=header-anchor href=#%e7%bb%93%e6%9e%84%e5%8c%96%e5%89%aa%e6%9e%9d%e4%b8%8e%e9%9d%9e%e7%bb%93%e6%9e%84%e5%8c%96%e5%89%aa%e6%9e%9d></a>结构化剪枝与非结构化剪枝</h4><p>结构化稀疏对非零权值的位置进行了限制，在剪枝过程中会将一些数值较大的权值剪枝。</p><p>这是一种权衡：非结构化稀疏具有更高的模型压缩率和准确性，但通用性不好。因为其计算特征上的“不规则”，导致需要特定硬件支持才能实现加速效果。结构化稀疏虽然牺牲了模型压缩率或准确率，但通用性更好。因为结构化稀疏使得权值矩阵更规则更加结构化，更利于硬件加速。</p><h3 id=结构化剪枝><a class=header-anchor href=#%e7%bb%93%e6%9e%84%e5%8c%96%e5%89%aa%e6%9e%9d></a>结构化剪枝</h3><p>复用已有的卷积、矩阵乘算子，无需实现推理算子。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>prune<span style=color:#f92672>.</span>ln_structured(module,name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;weight&#34;</span>,amount<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>,n<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#75715e># n代表范数类别， dim表示剪枝的维度，dim=0 表示按输出通道剪</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>index_remove</span>(tensor,dim,index):
</span></span><span style=display:flex><span>	size_ <span style=color:#f92672>=</span> list(tensor<span style=color:#f92672>.</span>size())
</span></span><span style=display:flex><span>	a <span style=color:#f92672>=</span> tensor<span style=color:#f92672>.</span>size(dim)
</span></span><span style=display:flex><span>	b <span style=color:#f92672>=</span> len(index)
</span></span><span style=display:flex><span>	new_size <span style=color:#f92672>=</span> a <span style=color:#f92672>-</span> b  <span style=color:#75715e># 计算移除索引后该维度的新大小</span>
</span></span><span style=display:flex><span>	size_[dim] <span style=color:#f92672>=</span> new_size <span style=color:#75715e># 更新 size_ 列表中对应维度的值，用于记录新张量的形状。</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># 例如set(range(3)) - set(1) = [0, 2]</span>
</span></span><span style=display:flex><span>	select_index <span style=color:#f92672>=</span> list(set(range(tensor<span style=color:#f92672>.</span>size(dim))) <span style=color:#f92672>-</span> set(index))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># 选择dim上剩下的行进行保留</span>
</span></span><span style=display:flex><span>	new_tensor <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>index_select(tensor,dim,torch<span style=color:#f92672>.</span>tensor(select_index))
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> new_tensor
</span></span></code></pre></div><blockquote><p>如何实现结构化和非结构化的剪枝？</p></blockquote><p>对于卷积核的重要性而言，可以使用<code>L1-norm</code> ，<code>L1</code>范数是向量中的各个元素之和。
<img src=output_image/76ecf1da8f86bfd3253fcc75c8c1a52d.png alt></p><p>重新定义网络的每一层，生成一个更小的模型结构。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_new_conv</span>(conv,dim,channel_index,independent_prune_flag<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>):
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>if</span> dim <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>		new_conv <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Conv2d(in_channels<span style=color:#f92672>=</span>conv<span style=color:#f92672>.</span>in_channels,
</span></span><span style=display:flex><span>			outchannels<span style=color:#f92672>=</span>int(conv<span style=color:#f92672>.</span>out_channels <span style=color:#f92672>-</span> len(channel_index)),
</span></span><span style=display:flex><span>			kernel_size<span style=color:#f92672>=</span>conv<span style=color:#f92672>.</span>kernel_size,
</span></span><span style=display:flex><span>			stride<span style=color:#f92672>=</span>conv<span style=color:#f92672>.</span>stride,padding<span style=color:#f92672>=</span>conv<span style=color:#f92672>.</span>paddding,dilation<span style=color:#f92672>=</span>conv<span style=color:#f92672>.</span>dilation)
</span></span><span style=display:flex><span>	new_conv<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> index_remove(conv<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>data, dim, channel_index)d
</span></span><span style=display:flex><span>	new_conv<span style=color:#f92672>.</span>bias<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> index_remove(conv<span style=color:#f92672>.</span>bias<span style=color:#f92672>.</span>data, channel_index)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> new_conv
</span></span></code></pre></div><p><img src=output_image/8cb4944d83a14b913d58e098ac5e7bc1.png alt>
需要指明的是，在多通道多卷积核的情况下，每一个卷积核$a_{i}$是对于单通道$b_{i}$进行卷积。输入channel，输出也是<code>channel</code>。</p><h3 id=正则化><a class=header-anchor href=#%e6%ad%a3%e5%88%99%e5%8c%96></a>正则化</h3><p>如果原始模型滤波器表现为0值的参数数量较小，则上述算法可供剪除的滤波器数目也将随之减小，从而达不到模型预期的压缩率。</p><h2 id=激活稀疏><a class=header-anchor href=#%e6%bf%80%e6%b4%bb%e7%a8%80%e7%96%8f></a>激活稀疏</h2><p>与权重稀疏的区别是：其是动态的，通过数据驱动的激活稀疏。
<img src=output_image/e4b3affc1afd84a811863d95dddd39d0.png alt></p><p>有点trick。对于一个神经元而言，对用于验证的图像样本、每个特征图维度进行$f(O_{c,j}(k)==0)$的情况进行求和，算百分比，定义为$APoZ$的值。那些激活输出APoZ指标接近于1的神经元，可以移除，不会显著影响整个网络的精度。</p><p><img src=output_image/de81829bb7f6a3a3fc249f07a1736041.png alt></p><ul><li>激活稀疏的引入使得DNN网络中存在大量的稀疏与冗余性。</li><li>激活稀疏是一种数据驱动的剪枝方法——神经元剪枝</li></ul><h2 id=梯度稀疏><a class=header-anchor href=#%e6%a2%af%e5%ba%a6%e7%a8%80%e7%96%8f></a>梯度稀疏</h2><p>在分布式训练中，多台节点上分别运行分布式随机梯度下降SGD算法，需要实时交换各自梯度计算数值，随着节点的增多，网络上传输的梯度数据量急剧增加，逐渐超过梯度计算本身消耗的资源。</p><p>梯度稀疏SGD算法：只有梯度大于<code>threshold</code>才被传输，剩下未被大于梯度的值则被local保存用于做累加直到大于<code>threshold</code>才被传输。结束循环时，一次性传播所有的梯度。</p><p><img src=output_image/b92e7c108dec366ca22b57ce7cfbf27a.png alt></p><p>小结：
<img src=output_image/1dc058ec2e0547f9d0fdb1cf2f304554.png alt></p><h2 id=量化低比特计算><a class=header-anchor href=#%e9%87%8f%e5%8c%96%e4%bd%8e%e6%af%94%e7%89%b9%e8%ae%a1%e7%ae%97></a>量化/低比特计算</h2><p>模型量化是一种将浮点数计算转成低比特计算的技术，可以有效降低模型计算的强度、参数大小和内存消耗，但往往带来巨大的精度损失。
<img src=output_image/da34f3798ca77dee5b171c342b0deee3.png alt></p><h2 id=三值网络二值网络><a class=header-anchor href=#%e4%b8%89%e5%80%bc%e7%bd%91%e7%bb%9c%e4%ba%8c%e5%80%bc%e7%bd%91%e7%bb%9c></a>三值网络/二值网络</h2><p><img src=output_image/99523033ef72e67e415b7d3995031878.png alt></p><h2 id=轻量化网络><a class=header-anchor href=#%e8%bd%bb%e9%87%8f%e5%8c%96%e7%bd%91%e7%bb%9c></a>轻量化网络</h2><h3 id=模型复杂度分析><a class=header-anchor href=#%e6%a8%a1%e5%9e%8b%e5%a4%8d%e6%9d%82%e5%ba%a6%e5%88%86%e6%9e%90></a>模型复杂度分析</h3><ul><li>$FLOPs$：计算量，可以用来衡量算法/模型时间的复杂度。</li><li>$FLOPS$：芯片每秒执行的浮点运算次数。</li><li>$MACCs$：乘加操作次数，大约是$FLOPs$的一半。</li><li>$MAC$：内存访问代价，指的是输入单个样本，模型/卷积层完成一次前向传播所发生的内存交换总量，即模型的空间复杂度，单位是byte。</li></ul><p>什么是轻量化网络MobileNet？
<img src=output_image/3a1bcf5eccdde2105ab6f1cc27be1866.png alt>
<img src=output_image/1c2bd286de7f80ecfaa68545185a205d.png alt>
上图的MAC大概是计算了特征图的参数量，并没有什么“内存交换”的意味。</p><p>什么是轻量化网络<em>shuffleNet</em>？
<img src=output_image/ae0fe647438c4fc2e4940f294c4b3b91.png alt>
上图中的$\frac{c_{in}}{g}$相当于Mobilenet的M，$\frac{c_{out}}{g}$相当于是Mobilenet的N。
分组之后各组之间信息互不相通，降低了网络的特征提取能力。对channel进行shuffle，增加了不同channels之间的信息连接多样性。</p><h1 id=分布式算法><a class=header-anchor href=#%e5%88%86%e5%b8%83%e5%bc%8f%e7%ae%97%e6%b3%95></a>分布式算法</h1><p>并行化的基本方案在下图的各个标题之中。</p><h2 id=算子内并行><a class=header-anchor href=#%e7%ae%97%e5%ad%90%e5%86%85%e5%b9%b6%e8%a1%8c></a>算子内并行</h2><p>并行单个张量计算子内的计算（GPU多处理单元并行）：</p><ul><li>利用线性计算和卷积等操作内部的并行性，在batch 空间维度 时间维度等多个并行维度上计算。利用SIMD架构等多执行单元，同时运算。</li></ul><h2 id=算子间并行><a class=header-anchor href=#%e7%ae%97%e5%ad%90%e9%97%b4%e5%b9%b6%e8%a1%8c></a>算子间并行</h2><p>数据并行：多个样本并行执行。
模型并行：多个算子并行执行。
组合并行：多种并行方案组合叠加。</p><h2 id=模型并行><a class=header-anchor href=#%e6%a8%a1%e5%9e%8b%e5%b9%b6%e8%a1%8c></a>模型并行</h2><p>例如，将卷积层部署到不同的GPU。标签应与模型输出位于相同的GPU。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>seq1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>conv1,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>bn1,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>relu,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>maxpool,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>layer1,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>layer2
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#39;cuda0&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>seq2 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>conv1,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>bn1,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>relu,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>maxpool,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>layer1,
</span></span><span style=display:flex><span>	self<span style=color:#f92672>.</span>layer2
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#39;cuda1&#39;</span>)
</span></span></code></pre></div><p>结果：并未减少训练时间的一半，甚至有时会更长，原因是：
<img src=output_image/6c9e59a9b4244d47d36ab24e5fd7031d.png alt></p><h3 id=模型并行的优化措施流水线><a class=header-anchor href=#%e6%a8%a1%e5%9e%8b%e5%b9%b6%e8%a1%8c%e7%9a%84%e4%bc%98%e5%8c%96%e6%8e%aa%e6%96%bd%e6%b5%81%e6%b0%b4%e7%ba%bf></a>模型并行的优化措施——流水线</h3><p>将<code>batch</code>进行切分，前一个<code>split</code>出<code>cuda0</code>时进入<code>cuda1</code>时，下一个<code>split</code>就可以进入<code>cuda0</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PipelineParallelRedNet50</span>(ModeParallelResnet50):
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>def</span> __init__(self,split_size<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,<span style=color:#f92672>**</span>args,<span style=color:#f92672>**</span>kwargs):
</span></span><span style=display:flex><span>		super(PipelineParallel)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>args,<span style=color:#f92672>**</span>kwargs)
</span></span><span style=display:flex><span>		self<span style=color:#f92672>.</span>split_size <span style=color:#f92672>=</span> split_size
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self,):
</span></span><span style=display:flex><span>		splits <span style=color:#f92672>=</span> iter(x<span style=color:#f92672>.</span>split(self<span style=color:#f92672>.</span>self<span style=color:#f92672>.</span>split_size,dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>		prev <span style=color:#f92672>=</span> next(splits)
</span></span><span style=display:flex><span>		s_prev <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>seq1(prev)<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#34;cuda0&#34;</span>)
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>for</span> s_next <span style=color:#f92672>in</span> splits:
</span></span><span style=display:flex><span>			s_prev <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>seq2(s_prev)
</span></span><span style=display:flex><span>			ret<span style=color:#f92672>.</span>append(self<span style=color:#f92672>.</span>fc(s_prev<span style=color:#f92672>.</span>view(s_prev<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>),<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>			s_prev <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>seq1(s_next)<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#34;cuda1&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		s_prev <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>seq2(s_prev)
</span></span><span style=display:flex><span>		ret<span style=color:#f92672>.</span>append(self<span style=color:#f92672>.</span>fc(s_prev<span style=color:#f92672>.</span>view(s_prev<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>),<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)))
</span></span></code></pre></div><h3 id=gpipe算法><a class=header-anchor href=#gpipe%e7%ae%97%e6%b3%95></a>Gpipe算法</h3><p>通过对batch进行拆分，减少设备空闲，更好地利用设备进行流水化。</p><p><strong>特点</strong>：每个batch的所有splits前馈运算(经过$Device\quad n$)结束后，才能开始后向运算。
<img src=output_image/4d682d036a2041776e0ed69880841fcd.png alt></p><h3 id=pipedream算法><a class=header-anchor href=#pipedream%e7%ae%97%e6%b3%95></a>PipeDream算法</h3><ul><li>第一个split前馈运算一旦结束，即提前开始后向运算</li><li>每个split的后向运算占两个时隙，从而人为制造出bubble，使得后续的前馈运算可以在此间隙中进行，从而消除了bubble。</li></ul><p>结果就是在Gpipe的基础上进一步减少了设备空闲。
<img src=output_image/a01553f756c986612e153b5b54596ff7.png alt></p><h2 id=数据并行><a class=header-anchor href=#%e6%95%b0%e6%8d%ae%e5%b9%b6%e8%a1%8c></a>数据并行</h2><p>即在不同设备上执行相同的计算图，跨设备聚合梯度，利用聚合后梯度更新模型。</p><h3 id=一对多><a class=header-anchor href=#%e4%b8%80%e5%af%b9%e5%a4%9a></a>一对多</h3><p>可以分为scater分发和broadcast广播。
<img src=output_image/9aebb18d9c5a6dac3e6f7b36c7b29503.png alt></p><h3 id=多对一><a class=header-anchor href=#%e5%a4%9a%e5%af%b9%e4%b8%80></a>多对一</h3><p>分为reduce整合和gather收集。
<img src=output_image/fa1e30a6d55a8a1bb51df7398176cf41.png alt></p><h3 id=多对多><a class=header-anchor href=#%e5%a4%9a%e5%af%b9%e5%a4%9a></a>多对多</h3><p>分为All Gather和All Reduce。
<img src=output_image/df021268af3e44bbecc50fccf0240294.png alt>
<strong>通信原语</strong>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist<span style=color:#f92672>.</span>broadcast(tensor,src,group)
</span></span><span style=display:flex><span>dist<span style=color:#f92672>.</span>scatter(tensor,scatter_list,src,group) <span style=color:#75715e># copy the i tensor scatter_list[i] to the i process</span>
</span></span><span style=display:flex><span>dist<span style=color:#f92672>.</span>all_reduce(tensor,op,group) <span style=color:#75715e># same as reduce,but result is stored in all process </span>
</span></span><span style=display:flex><span>dist<span style=color:#f92672>.</span>gather(tensor,gather_list,dst,group) <span style=color:#75715e># copy tensor from all process in dst</span>
</span></span><span style=display:flex><span>dist<span style=color:#f92672>.</span>all_gather(tensor_list,tensor,group) <span style=color:#75715e># copy tensor from all process to tensor_list</span>
</span></span><span style=display:flex><span>dist<span style=color:#f92672>.</span>reduce(tensor,dst,op,group) <span style=color:#75715e># applies op to all tensor and stores the result in dst.</span>
</span></span><span style=display:flex><span>dist<span style=color:#f92672>.</span>barrier(group) <span style=color:#75715e>#block all processes in group until each one has entered this function.</span>
</span></span></code></pre></div><blockquote><p>Allreduce的基础是点对点通信。用以实现同步的<code>send/recv</code>，例如：<code>rank0 send -> rank 1 recv</code>,又称为阻塞式通信，发端需等待收端开始接收数据之后才能结束，发送命令的返回意味着收端已经执行了一定程度的接收工作。双方进程到达一个确定的同步点之后，通信才可以结束。</p></blockquote><blockquote><p>异步实现send/recv：又称非阻塞式通信，可以将通信和计算进行重叠，有时可以大大改善性能。 即发送命令并不要求操作立即执行，从发送数据区取走数据即可返回，在wait()收端接收完之前可以并发进行数据传输和运算。</p></blockquote><h3 id=allreduce-的通信原语实现算法1><a class=header-anchor href=#allreduce-%e7%9a%84%e9%80%9a%e4%bf%a1%e5%8e%9f%e8%af%ad%e5%ae%9e%e7%8e%b0%e7%ae%97%e6%b3%951></a>Allreduce 的通信原语实现算法1</h3><p><img src=output_image/0a58b07081685444d2fe3e871dc55bf9.png alt>
$\log _2 N$代表着时间上不重叠的通信次数，$\alpha$可以认为是一个固定时延，传输时间是$\frac{S}{B}$，$2\times S\times B$可以认为是接收节点的计算耗时。由于接收了两份数据所以乘2.</p><h3 id=allreduce-的通信原语实现算法2><a class=header-anchor href=#allreduce-%e7%9a%84%e9%80%9a%e4%bf%a1%e5%8e%9f%e8%af%ad%e5%ae%9e%e7%8e%b0%e7%ae%97%e6%b3%952></a>Allreduce 的通信原语实现算法2</h3><p><img src=output_image/95d6b0befe11d7504e0293ad9e733d78.png alt> 在<code>scatter-reduce</code>中，<code>A1 A2 A3 A4</code>分别发送到不同的<code>worker</code>上，A1、B1、C1和D1发送到相同的<code>worker</code>上，进而做<code>reduce</code>。</p><h3 id=ring-allreduce><a class=header-anchor href=#ring-allreduce></a>Ring Allreduce</h3><p>为了避免master的带宽限制，采用环状的reduce加上gather。 <img src=output_image/9ee15a5d014dfd4f3aaad8bb78723efa.png alt> 之后再经过N-1次的<code>gather</code>：
<img src=output_image/0b4327dade25df5c5bbdaae388a799c8.png alt></p><p>性能分析：在整个过程中每个<code>worker</code>上<code>send</code>或<code>receive</code>的总通信数据量均为：$\frac{2(N-1)K}{N}$。K代表每个<code>worker</code>上所有<code>chunk</code>的总数据量。$\frac{K}{N}$是每个<code>worker</code>上每一块的数据量。</p><p>每个$worker$通信数据量近似独立于网络中<code>worker</code>的数量，为$O(K)$，而主从架构中主节点的通信数据量为$O(N*K)$。</p><p>每个<code>worker</code>的网络收发负载是均衡的，网络双向带宽得到充分利用。</p><h4 id=程序实现><a class=header-anchor href=#%e7%a8%8b%e5%ba%8f%e5%ae%9e%e7%8e%b0></a>程序实现</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>allreduce</span>(send,recv):
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(size <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> i <span style=color:#f92672>%</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>			send_req <span style=color:#f92672>=</span> dist<span style=color:#f92672>.</span>isend(send_buff,right) <span style=color:#75715e># 非阻塞发送`send_buff`（初始为`send`的数据）给右侧进程（`right`）。 </span>
</span></span><span style=display:flex><span>			dist<span style=color:#f92672>.</span>recv(recv_buff,left) <span style=color:#75715e># 阻塞接收左侧进程（`left`）的数据到`recv_buff`。 </span>
</span></span><span style=display:flex><span>			accum[:] <span style=color:#f92672>+=</span> recv_buff[:] <span style=color:#75715e># 将接收到的`recv_buff`数据累加到`accum`。</span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>else</span> :
</span></span><span style=display:flex><span>			send_req <span style=color:#f92672>=</span> dist<span style=color:#f92672>.</span>isend(recv_buff,right) <span style=color:#75715e>#非阻塞发送`recv_buff`（上一轮接收的数据）给右侧进程。</span>
</span></span><span style=display:flex><span>			dist<span style=color:#f92672>.</span>recv (send_buff,left) <span style=color:#75715e># 阻塞接收左侧进程的新数据到`send_buff`。</span>
</span></span><span style=display:flex><span>			accum[:] <span style=color:#f92672>+=</span> send_buff[:]
</span></span><span style=display:flex><span>		send_req<span style=color:#f92672>.</span>wait() <span style=color:#75715e># 每次迭代结束前，调用`send_req.wait()`确保数据发送完成，避免通信重叠错误。</span>
</span></span><span style=display:flex><span>	recv[:] <span style=color:#f92672>=</span> accum[:]	
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run</span>(rank,size):
</span></span><span style=display:flex><span>	t <span style=color:#f92672>=</span> th<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>2</span>):
</span></span><span style=display:flex><span>		c <span style=color:#f92672>=</span> t<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>		allreduce(t,c) <span style=color:#75715e>#t发c收</span>
</span></span><span style=display:flex><span>		t<span style=color:#f92672>.</span>set_(c)  <span style=color:#75715e># 收到并且更新</span>
</span></span><span style=display:flex><span>	print(t)
</span></span></code></pre></div><p>阻塞代表预先接收数据，发送则在空闲的时候进行，从而避免冲突发生。</p><h3 id=通信后端><a class=header-anchor href=#%e9%80%9a%e4%bf%a1%e5%90%8e%e7%ab%af></a>通信后端</h3><ul><li>MPI 通用性强，适合复杂的HPC任务，支持异构环境，代表例子有Horovod</li><li>NCCL 英伟达2015年公开发布，充分发挥英伟达自身硬件性能，广泛用于GPU深度学习</li><li>GLoo Facebook开发，<code>pytorch</code>默认安装，跨平台，易用，适合 CPU 或低性能网络环境，在GPU中不如NCCL高效。</li></ul><h3 id=集合通信原语的应用分布式sgd算法><a class=header-anchor href=#%e9%9b%86%e5%90%88%e9%80%9a%e4%bf%a1%e5%8e%9f%e8%af%ad%e7%9a%84%e5%ba%94%e7%94%a8%e5%88%86%e5%b8%83%e5%bc%8fsgd%e7%ae%97%e6%b3%95></a>集合通信原语的应用：分布式SGD算法</h3><p>多态节点上分别运行分布式随机梯度下降算法，需要实时交换各自梯度计算数值。
利用的是全局加和后的梯度进行更新。</p><h3 id=单机多卡分布式sgd><a class=header-anchor href=#%e5%8d%95%e6%9c%ba%e5%a4%9a%e5%8d%a1%e5%88%86%e5%b8%83%e5%bc%8fsgd></a>单机多卡分布式SGD</h3><p>稍微看一下就行，老师上课也没有多讲什么。
<img src=output_image/73c0d1ee9e12ce5bca604a69274ff1ea.png alt>
<img src=output_image/c4a432633de14fe15d380944ced5f076.png alt></p><h3 id=多机分布式sgd><a class=header-anchor href=#%e5%a4%9a%e6%9c%ba%e5%88%86%e5%b8%83%e5%bc%8fsgd></a>多机分布式SGD</h3><p>首先为各个主机编写<code>shell</code>脚本：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python -m torch.distributed.launch <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --master_port <span style=color:#ae81ff>12345</span> <span style=color:#ae81ff>\ </span>     <span style=color:#75715e># 主节点监听的端口号</span>
</span></span><span style=display:flex><span>    --nproc_per_node<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\ </span>      <span style=color:#75715e># 每个节点启动的进程数（通常等于GPU数量）</span>
</span></span><span style=display:flex><span>    --nnodes<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> <span style=color:#ae81ff>\ </span>              <span style=color:#75715e># 参与训练的总节点数</span>
</span></span><span style=display:flex><span>    --node_rank<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\ </span>           <span style=color:#75715e># 当前节点的编号（从0开始）</span>
</span></span><span style=display:flex><span>    --master_addr<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;115.154.142.79&#34;</span> <span style=color:#ae81ff>\ </span> <span style=color:#75715e># 主节点的IP地址</span>
</span></span><span style=display:flex><span>    test_module.py             <span style=color:#75715e># 要执行的训练脚本</span>
</span></span></code></pre></div><p>其次：使用<code>parser.add_argumet</code>加入分布式支持。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--tcp&#39;</span>,type<span style=color:#f92672>=</span>str,default<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&#34;</span>,metavar<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;N&#39;</span>,help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;asda&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 等等相关参数……</span>
</span></span><span style=display:flex><span>parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--world_size&#39;</span>,default<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,type<span style=color:#f92672>=</span>int,help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;number of distributed process&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#39;--gpu&#39;</span>,type<span style=color:#f92672>=</span>int,default<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,metavar<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;N&#39;</span>,help<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;distributed rank&#39;</span>)
</span></span></code></pre></div><ul><li><code>metavar='N'</code> 表示在帮助信息（<code>--help</code>）中，<code>--gpu</code> 选项后会显示为 <code>N</code>，提示用户此处需要输入一个整数值。</li></ul><p>进而：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>args<span style=color:#f92672>.</span>rank <span style=color:#f92672>=</span> int(os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;RANK&#34;</span>])
</span></span><span style=display:flex><span>args<span style=color:#f92672>.</span>world_size <span style=color:#f92672>=</span> int(os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;WORLD_SIZE&#34;</span>])
</span></span><span style=display:flex><span>args<span style=color:#f92672>.</span>gpu <span style=color:#f92672>=</span> int(os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;LOCAL_RANK&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dist<span style=color:#f92672>.</span>init_process_group(init_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;env://&#39;</span>,backend<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;nccl&#34;</span>,rank<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>rank,world_size<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>world_size,group_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pytorch_test&#34;</span>)
</span></span></code></pre></div><p>最后：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_sampler <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>distributed<span style=color:#f92672>.</span>DistributedSampler(train_dataset)
</span></span></code></pre></div><h3 id=具有同步障的dist-sgd算法><a class=header-anchor href=#%e5%85%b7%e6%9c%89%e5%90%8c%e6%ad%a5%e9%9a%9c%e7%9a%84dist-sgd%e7%ae%97%e6%b3%95></a>具有同步障的dist-SGD算法</h3><p><img src=output_image/794ea410abf27367825d28829c401945.png alt></p><h3 id=horovod><a class=header-anchor href=#horovod></a>Horovod</h3><blockquote><p>Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. The goal of Horovod is to make distributed deep learning fast and easy to use.
<a href=https://github.com/horovod/horovod>horovod/horovod: Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.</a></p></blockquote><p>让代码不再依赖torch.distribute的支持，将基本的并行命令进一步封装于底层，对用户透明，更简单易用。提供梯度压缩等提升数据并行效率的函数支持，以及提供多种通信后端的支持。</p><h2 id=小结><a class=header-anchor href=#%e5%b0%8f%e7%bb%93></a>小结</h2><p><img src=output_image/d3f5036bb0cc5b3eee41e0d5394f815b.png alt></p><h1 id=分布式训练-实验部分><a class=header-anchor href=#%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83-%e5%ae%9e%e9%aa%8c%e9%83%a8%e5%88%86></a>分布式训练 实验部分</h1><p>通信后端:<code>MPI</code> <code>NCCL</code> <code>Gloo</code>.</p><p>我们知道 PyTorch 本身对于单机多卡提供了两种实现方式:</p><ul><li>DataParallel（DP）：nn.Dataparallel 方法实际上是使用单进程将模型和数据加载到多个 GPU 上,协同不同的 GPU 上的模型进行并行训练。</li><li>DistributedDataParallel（DDP）：与 nn.Dataparallel 使用单进程控制多个 GPU 不同， nn.DistributedDataParallel 为每个 GPU 都创建一个进程。这些 GPU 可以位于同一个结点上（单机多卡），也可以分布在多个节点上（多机多卡）。每个进程都执行相同的任务，每个进程都与其他进程进行通信。一点不同是，只有梯度会在进程（GPU）之间传播。以单机多卡举例，假设我们有三张卡并行训练，那么在每个 epoch 中，数据集会被划分成三份给三个 GPU，每个 GPU 使用自己的 minibatch 数据做自己的前向计算，然后梯度在 GPU 之间全部约简。在反向传播结束的时候，每个 GPU 都有平均的梯度，确保模型权值保持同步（synchronized）。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>model <span style=color:#f92672>=</span> nn.DataParallel<span style=color:#f92672>(</span>model, device_ids<span style=color:#f92672>=</span>config.gpu_id, output_device<span style=color:#f92672>=</span>gpu<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># DDP</span>
</span></span><span style=display:flex><span>from torch.nn.parallel import DistributedDataParallel as DDP
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>def setup<span style=color:#f92672>(</span>rank,world_size<span style=color:#f92672>)</span>:
</span></span><span style=display:flex><span>	os.environ<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;MASTER_ADDR&#39;</span><span style=color:#f92672>]</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;localhost&#39;</span>
</span></span><span style=display:flex><span>	os.environ<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;MASTER_PORT&#39;</span><span style=color:#f92672>]</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;12345&#39;</span> 
</span></span><span style=display:flex><span>	dist.init_process_group<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;nccl&#34;</span>, init_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tcp://127.0.0.1:12345&#39;</span>,rank<span style=color:#f92672>=</span>rank, world_size<span style=color:#f92672>=</span>world_size<span style=color:#f92672>)</span> <span style=color:#75715e># 在 DDP 管理的进程组中，每个独立的进程还需要知道进程组中管理进程的数量，我们称为 world_size</span>
</span></span><span style=display:flex><span><span style=color:#75715e># rank是当前进程的等级。在 DDP 管理的进程组中，每个独立的进程需要知道自己在所有进程中的阶序，我们称为 rank.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># init_method=&#39;tcp://127.0.0.1:12345&#39; 一般是单机多卡这样配置</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>def run<span style=color:#f92672>(</span>rank,world_size<span style=color:#f92672>)</span>:
</span></span><span style=display:flex><span>	setup<span style=color:#f92672>(</span>rank,world_size<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>	device <span style=color:#f92672>=</span> torch.device<span style=color:#f92672>(</span>f<span style=color:#e6db74>&#39;cuda:{rank}&#39;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>	model <span style=color:#f92672>=</span> mymodel<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>	model <span style=color:#f92672>=</span> DDP<span style=color:#f92672>(</span>model,device_ids<span style=color:#f92672>=[</span>rank<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>	optimizer <span style=color:#f92672>=</span> optim.SGD<span style=color:#f92672>(</span>……<span style=color:#f92672>)</span> 
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> epoch in epoches:
</span></span><span style=display:flex><span>			
</span></span><span style=display:flex><span>def main<span style=color:#f92672>()</span>:
</span></span><span style=display:flex><span>	torch.multiprocessing.spawn<span style=color:#f92672>(</span>train,args<span style=color:#f92672>=(</span>world_size,<span style=color:#f92672>)</span>,nprocs<span style=color:#f92672>=</span>world_size,join<span style=color:#f92672>=</span>True<span style=color:#f92672>)</span> <span style=color:#75715e># 这里的train是void*，指向一个方法 ，在这个方法中，需要调用ddp_model = DDP(model, device_ids=[rank])</span>
</span></span></code></pre></div><blockquote><p>问：你是如何理解并行/分布式训练原理的？
答：并行分为数据并行和模型并行，数据并行将数据切分成多个子集，每个设备处理一个数据子集，每个设备拥有完整的模型副本。模型并行将模型切分到不同设备上,每个模型负责一部分计算。</p></blockquote><blockquote><p>问：你是如何执行分布式训练的？
答： 我执行的是单机多卡分布式训练。</p></blockquote><p>首先利用torchrun，自动生成进程的rank = nproc_per_node * node_rank + local_rank，对于单机多卡而言，torchrun使得每个node中local_rank从$0,1,……，n-1$递增，自动求得每个进程的rank。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 单机多卡</span>
</span></span><span style=display:flex><span>torchrun --standalone --nproc_per_node <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>	mynet.py
</span></span></code></pre></div><p>接着 在<code>setup</code>中让<code>torchrun</code>自动定义<code>master_addr master_port</code>：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>setup</span>(rank):
</span></span><span style=display:flex><span>	 init_process_group(<span style=color:#e6db74>&#34;nccl&#34;</span>,intial_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;env://&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>	rank <span style=color:#f92672>=</span> int(os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#34;LOCAL_RANK&#34;</span>])
</span></span><span style=display:flex><span>	setup(rank)
</span></span><span style=display:flex><span>	<span style=color:#75715e># 接着将dataset 进行切分</span>
</span></span><span style=display:flex><span>	dataset <span style=color:#f92672>=</span> Mydataset()
</span></span><span style=display:flex><span>    sampler <span style=color:#f92672>=</span> DistributedSampler(dataset) <span style=color:#66d9ef>if</span> is_distributed <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>    
</span></span><span style=display:flex><span>    loader <span style=color:#f92672>=</span> Dataloader()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e># model to rank</span>
</span></span><span style=display:flex><span>	model<span style=color:#f92672>.</span>to(rank)
</span></span><span style=display:flex><span>	model <span style=color:#f92672>=</span> DDP(model,device_id<span style=color:#f92672>=</span>[rank])
</span></span><span style=display:flex><span>	train(<span style=color:#960050;background-color:#1e0010>……</span>)
</span></span><span style=display:flex><span>	
</span></span></code></pre></div><p>多机多卡：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 多机多卡</span>
</span></span><span style=display:flex><span>torchrun <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --nnodes<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span> <span style=color:#ae81ff>\ </span>              <span style=color:#75715e># 总节点数</span>
</span></span><span style=display:flex><span>    --nproc_per_node<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span> <span style=color:#ae81ff>\ </span>      <span style=color:#75715e># 每节点GPU数 进程数 world_size = nproc_per_node * nnodes </span>
</span></span><span style=display:flex><span>    --node_rank<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> <span style=color:#ae81ff>\ </span>           <span style=color:#75715e># 当前节点rank </span>
</span></span><span style=display:flex><span>    --master_addr<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;192.168.1.100&#34;</span> <span style=color:#ae81ff>\ </span> <span style=color:#75715e># 主节点IP</span>
</span></span><span style=display:flex><span>    --master_port<span style=color:#f92672>=</span><span style=color:#ae81ff>12345</span> <span style=color:#ae81ff>\ </span>     <span style=color:#75715e># 主节点端口</span>
</span></span><span style=display:flex><span>    train_script.py
</span></span></code></pre></div><h1 id=异构计算集群调度与资源管理系统><a class=header-anchor href=#%e5%bc%82%e6%9e%84%e8%ae%a1%e7%ae%97%e9%9b%86%e7%be%a4%e8%b0%83%e5%ba%a6%e4%b8%8e%e8%b5%84%e6%ba%90%e7%ae%a1%e7%90%86%e7%b3%bb%e7%bb%9f></a>异构计算集群调度与资源管理系统</h1><h2 id=docker的介绍><a class=header-anchor href=#docker%e7%9a%84%e4%bb%8b%e7%bb%8d></a>Docker的介绍</h2><p><code>docker</code>镜像分层构建的，分层结构的目的之一是共享资源，如多个镜像从相同的<code>base</code>镜像构建而来，则只需在磁盘上保存一份base镜像；内存中加载的一份该<code>base</code>镜像可以为所有容器服务。例如典型 <code>Linux</code>文件系统由 <code>bootfs</code> 和<code>rootfs</code> 两部分组成，<code>Docker</code>将其他<code>layer</code>加载其上，且每层都是只读<code>rootfs</code>结构，相同的<code>base</code>镜像可以共享，只有顶层的容器层可以读写。</p><blockquote><p>$容器=镜像image+读写层$</p></blockquote><p>Docker的网络配置分为<code>container bridge host</code>三种，具体可以参考：<a href=https://zhuanlan.zhihu.com/p/212772001>Docker 网络模式详解及容器间网络通信 - 知乎</a>。</p><h2 id=镜像与容器实践><a class=header-anchor href=#%e9%95%9c%e5%83%8f%e4%b8%8e%e5%ae%b9%e5%99%a8%e5%ae%9e%e8%b7%b5></a>镜像与容器实践</h2><p><img src=output_image/5653af0c9ee76458281dca8cac641e8f.png alt>
PPT上的docker 命令行：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker build -f Dockerfile -t train_cpu:latest .
</span></span><span style=display:flex><span>docker run --name training<span style=color:#f92672>(</span>容器名称） train_cpu<span style=color:#f92672>(</span>镜像名称<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>docker pull &lt;name&gt;
</span></span><span style=display:flex><span>sudo docker run -it &lt;name&gt; /bin/sh
</span></span></code></pre></div><h2 id=公平性调度><a class=header-anchor href=#%e5%85%ac%e5%b9%b3%e6%80%a7%e8%b0%83%e5%ba%a6></a>公平性调度</h2><ul><li>最大最小公平（max-min fairness）算法—— the most popular allocation policy so far.Idea： Maximizes the minimum allocation received by a user in the system.</li><li>Dominant Resource Fairness (DRF).Its goal is to maximizes the minimum dominant share across all users.</li></ul><h3 id=异构资源的公平性调度><a class=header-anchor href=#%e5%bc%82%e6%9e%84%e8%b5%84%e6%ba%90%e7%9a%84%e5%85%ac%e5%b9%b3%e6%80%a7%e8%b0%83%e5%ba%a6></a>异构资源的公平性调度</h3><p>对于增加的比例而言，需要使主导资源的比例equal，如下图：
<img src=output_image/a392ac8700ef07f7f3811db9f73decdb.png alt>
在这个例子中，$y$先取1，所以user B 的dom share就是$\frac{1}{3}$，接着x取1，所以user A的dom share就是$\frac{2}{9}$，相比之下$\frac{2}{9} \lt \frac{1}{3}$，所以再次增加user A的资源，取$x = 2$，来到了$\frac{4}{9}$。
$<\frac{1}{9},\frac{4}{18}>$ 中，选择较大的那一个作为user占用内存的计量值。
<img src=output_image/ef0a395671875fbc6a1361f17bb1900b.png alt></p><h3 id=拓扑亲和性><a class=header-anchor href=#%e6%8b%93%e6%89%91%e4%ba%b2%e5%92%8c%e6%80%a7></a>拓扑亲和性</h3><p><strong>共享异常（Sharing anomaly)</strong>：
Quota机制为每个任务指定所需要使用GPU的数目，即使分配了足够的GPU数目，由于不同用户随时可能会要求分配或者释放GPU数目，就会导致出现大量的外部碎片，即随着分配、释放的次数变多，大量的GPU数目可能会变成跨CPU socket、甚至是跨机器。因此，造成了虽然分配了足够的GPU，但是还是会导致性能的大幅下降。</p><h3 id=hived调度算法><a class=header-anchor href=#hived%e8%b0%83%e5%ba%a6%e7%ae%97%e6%b3%95></a>HiveD调度算法</h3><p>分层级的，尽可能先从high-level分配资源满足租户的需求。
<img src=output_image/52ae2bd5ccbaf9fd2501c15a69998385.png alt></p><h2 id=调度的灵活性弹性与抢占><a class=header-anchor href=#%e8%b0%83%e5%ba%a6%e7%9a%84%e7%81%b5%e6%b4%bb%e6%80%a7%e5%bc%b9%e6%80%a7%e4%b8%8e%e6%8a%a2%e5%8d%a0></a>调度的灵活性——弹性与抢占</h2><h3 id=hadoop-yarn抢占的实现><a class=header-anchor href=#hadoop-yarn%e6%8a%a2%e5%8d%a0%e7%9a%84%e5%ae%9e%e7%8e%b0></a>Hadoop YARN抢占的实现</h3><ul><li>step 1:从过度使用的队列中获取需要被抢占的容器</li><li>step2：通知application master以便调度器采取行动，上报组织</li><li>step3：resource manager 强行杀死被标记为抢占的容器
这些在<code>HiveD</code>上是这样的：</li><li>允许优先级高的任务能够抢占优先级低的任务所需要的单元(cell)。</li><li>低优先级单元一般被分配于远离高优先级绑定的区域（如尽量避免分配buddy单元），以降低被抢占几率。</li><li>高优先级作业也尽量避免在存在低优先级作业的设备上分配单元。</li></ul><h2 id=异构计算集群管理系统><a class=header-anchor href=#%e5%bc%82%e6%9e%84%e8%ae%a1%e7%ae%97%e9%9b%86%e7%be%a4%e7%ae%a1%e7%90%86%e7%b3%bb%e7%bb%9f></a>异构计算集群管理系统</h2><p>OpenPAI（Open Platform for AI）， 是由MSRA开发的人工智能集群管理平台——分布式训练环境下的操作系统。</p><ul><li>基于Docker的架构，几乎所有深度学习框架如CNTK、 TensorFlow、PyTorch等无需修改即可运行。</li><li>作业请求提交至Hadoop YARN进行资源分配与调度</li><li>OpenPAI给YARN添加了使GPU作为可计算资源调度</li><li>静态资源由Kubernetes进行管理</li></ul><h1 id=docker的使用与解释-实验部分><a class=header-anchor href=#docker%e7%9a%84%e4%bd%bf%e7%94%a8%e4%b8%8e%e8%a7%a3%e9%87%8a-%e5%ae%9e%e9%aa%8c%e9%83%a8%e5%88%86></a>Docker的使用与解释 实验部分</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo docker build -f Dockerfile.train.cpu -t train_dk_cpu .
</span></span><span style=display:flex><span>docker images <span style=color:#75715e># 查看系统中的镜像</span>
</span></span><span style=display:flex><span>sudo docker run --name training train_dk_cpu
</span></span><span style=display:flex><span>docker ps <span style=color:#75715e># 查看当前正在运行的镜像</span>
</span></span></code></pre></div><h2 id=示例dockerfile><a class=header-anchor href=#%e7%a4%ba%e4%be%8bdockerfile></a>示例dockerfile</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>ARG</span> CODE_VERSION<span style=color:#f92672>=</span>latest
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 继承自哪个基础镜像 FROM前面只能有一个或多个ARG指令，dockerfile只能从FROM开始（除去ARG）</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> nvidia/cuda:10.1-cudnn7-devel</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>ENV</span> cuda<span style=color:#f92672>=</span><span style=color:#ae81ff>11</span>.3.1<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 创建镜像中的文件夹，用于存储新的代码或文件</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> mkdir -p /src/app<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># WORKDIR指令设置Dockerfile中的任何RUN，CMD，ENTRPOINT，COPY和ADD指令的工作目录，WORKDIR并不会在容器中创建目录，只是设置工作目录，如果没有，docker会自动创建。</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>WORKDIR</span><span style=color:#e6db74> /src/app</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 在设置了WORKDIR之后，COPY pytorch_mnist_basic.py /src/app可以简化为COPY pytorch_mnist_basic.py . 命令</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># ADD可以将本地文件添加到容器中，也可以从远程URL下载文件并且添加到容器中。如果文件是压缩文件则会自动解压缩。</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>ADD</span> nickdir.tar.gz .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 拷贝本地文件到Docker镜像中相应目录，注意是本地。不能从远程url中下载文件，也不会自动解压缩</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> pytorch_mnist_basic.py /src/app<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 用于容器内部的端口暴露给外部网络，让其他容器主机上的程序通过网络连接到容器内运行的服务。EXPOSE指令并不会将容器内部上的端口映射到主机上的端口，因此如果需要从主机上访问容器内部的服务，需要使用docker run -p 进行端口映射。</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>EXPOSE</span><span style=color:#e6db74> 80/tcp</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 需要安装的依赖，每个指令会被当做一个新的镜像层。每个RUN</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> apt-get update <span style=color:#f92672>&amp;&amp;</span> apt-get install wget -y<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> bash miniconda.sh -b -p /opt/conda<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 下面的RUN命令被解析为json数组，必须使用双引号而非单引号。</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> <span style=color:#f92672>[</span><span style=color:#e6db74>&#34;executable&#34;</span>,<span style=color:#e6db74>&#34;param1&#34;</span>,<span style=color:#e6db74>&#34;param2&#34;</span><span style=color:#f92672>]</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e>#ENV 设置环境变量</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>ENV</span> PATH /opt/conda/bin:$PATH <span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> conda install pytorch torchvision cudatoolkit<span style=color:#f92672>=</span>10.1 -c pytorch<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 在容器中运行命令时使用的用户或用户组，增强容器的安全性。</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>USER</span><span style=color:#e6db74> Bertsin</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 在容器中创建挂载点，将宿主机上的目录与容器内的目录进行映射，以便容器内部与外部共享数据。</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>VOLUME</span> [<span style=color:#e6db74>&#34;/data&#34;</span>] <span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 容器启动时执行的命令，dockerfile种只能有一个CMD命令，如果有多个，则最后一个才生效。</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>CMD</span> [ <span style=color:#e6db74>&#34;python&#34;</span>, <span style=color:#e6db74>&#34;pytorch_mnist_basic.py&#34;</span> ]<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># ENTRYPOINT [&#34;python&#34;,&#34;pytorch_mnist_basic.py&#34;]  设置一个默认应用程序，每次使用该镜像创建容器时都会使用该程序。可以在docker run 后指定--entrypoint选项，将覆盖ENTRYPOINT指令指定的程序。如果Dockerfile中同时指定了ENTRYPOINT和CMD指令，CMD指令提供的命令会作为ENTRYPOINT指定的命令的参数。如果运行容器时提供了参数，则会覆盖CMD指令提供的参数。</span><span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><p><code>CMD</code>和<code>ENTRYPOINT</code>都可以指定容器启动的时候要运行的命令。不同的是，<code>CMD</code>可以被<code>docker run cmd-test:0.1 &lt;命令></code>给覆盖。对于<code>ENTRYPOINT</code>而言，<code>docker run cmd-test:0.1 -l</code>，会在<code>ENTRYPOINT ["ls","-a"]</code>命令之后进行<code>append</code>，成为<code>al -la</code>。</p><h2 id=docker部署pytorch推理程序><a class=header-anchor href=#docker%e9%83%a8%e7%bd%b2pytorch%e6%8e%a8%e7%90%86%e7%a8%8b%e5%ba%8f></a>docker部署pytorch推理程序</h2><p>torchserve可以使大规模部署经过训练的pytorch更加轻松。包括以下组件：Model archiver将model打包成trochserve支持的格式。model loader用于加载和初始化打包好的模型。model predictor用于将输入数据传给模型进行预测，并返回预测结果。model-view controller 用于将请求路由到正确的模型示例，以及请求和响应。</p><h1 id=security--privacy><a class=header-anchor href=#security--privacy></a>Security & privacy</h1><ul><li>联邦学习：在不共享数据的情况下联合建模；</li><li>同态加密：允许在密文的基础上进行代数运算。</li></ul><h2 id=对抗攻击><a class=header-anchor href=#%e5%af%b9%e6%8a%97%e6%94%bb%e5%87%bb></a>对抗攻击</h2><p>攻击的损失函数</p><ul><li>训练时：${L_{train}}(\theta ) = C({y^0},{y^{true}})$，图像$x$是固定的</li><li>无目标攻击 $L({x_{false\_img}}) = - C({y^{false\_img\_pred}},{y^{true}})$ ，$\theta$是固定的。</li><li>有目标攻击$L({x_{false\_img}}) = - C({y^{false\_img\_pred}},{y^{true}}) + C({y^{false\_img\_pred}},{y^{false}})$，其中$y^{false}$是我想让Net预测错误的类别。</li><li>约束可以是L2范数，无穷范数。$d(x^0,x^{'})$
$$x^* = \arg \min L({x_{false\_img}})$$
上式中，$x^*$为找到的最终对抗样本。</li></ul><p>伪代码：</p><pre tabindex=0><code>start from original image x0
for t = 1 to T:
	xt = x_{t-1} - ksi * gradient(x_{t-1})
	if distance(x{0},x{t}) &gt; epsilon
		x_{t} = fix (x_t)

def fix(x_{t}):
	for all x fullfill distance(x,x{0}) &lt;= epsilon
	return the one closest to x_{t}
</code></pre><p><code>distance</code> 可以使用二范数或者无穷范数。$\epsilon$的大小决定了能够迭代多远。如果$\epsilon$过小，则会出现寻找假样本的第一步有效，之后T-1步均无效迭代的情况。</p><h3 id=攻击方法fgsm><a class=header-anchor href=#%e6%94%bb%e5%87%bb%e6%96%b9%e6%b3%95fgsm></a>攻击方法FGSM</h3>$${x_{i + 1}} = x_{i} - \varepsilon \cdot sign({\nabla _x}L(x,y;\theta ))$$<p>$x^{*} =argmin L(x_{false_{img}})$，这里的$argmin$是对$d(x^{0},x{'})\leq\epsilon$而言，在满足这个的条件下取loss最小的$x{'}$。</p><h3 id=白盒与黑盒><a class=header-anchor href=#%e7%99%bd%e7%9b%92%e4%b8%8e%e9%bb%91%e7%9b%92></a>白盒与黑盒</h3><p>白盒攻击：在进行攻击之前，就知道网络的模型参数$\theta$。</p><p>黑盒攻击：如果你有目标网络的训练数据，自己训练一个代理网络，使用代理网络生成对抗样本。若无训练数据，从目标网络获取输入输出对。</p><h2 id=通用对抗攻击><a class=header-anchor href=#%e9%80%9a%e7%94%a8%e5%af%b9%e6%8a%97%e6%94%bb%e5%87%bb></a>通用对抗攻击</h2><h3 id=对抗补丁><a class=header-anchor href=#%e5%af%b9%e6%8a%97%e8%a1%a5%e4%b8%81></a>对抗补丁</h3><p>全图加噪声扰动的方式需要操纵整个图像，并且在现实世界的物理攻击中变得很不现实，引入对抗补丁作为现实世界进行攻击的一种实用方法。它通过在目标物体表面添加一个经过特殊设计的局部图案（如贴纸、涂鸦或图案），使得机器学习模型（尤其是计算机视觉模型）对包含该补丁的输入产生错误判断。</p><h2 id=防御><a class=header-anchor href=#%e9%98%b2%e5%be%a1></a>防御</h2><h3 id=被动防御><a class=header-anchor href=#%e8%a2%ab%e5%8a%a8%e9%98%b2%e5%be%a1></a>被动防御</h3><p>在不修改模型的情况下找到对抗样本。对$x_{falseimg}$进行$smoothing$处理。</p><ul><li>特征减缩</li><li>随机调整数据尺寸</li></ul><h3 id=主动防御><a class=header-anchor href=#%e4%b8%bb%e5%8a%a8%e9%98%b2%e5%be%a1></a>主动防御</h3><p><img src=output_image/8b493a9f0fe2b0a35c37e54e099b39dd.png alt></p><h3 id=被动防御与主动防御的区别><a class=header-anchor href=#%e8%a2%ab%e5%8a%a8%e9%98%b2%e5%be%a1%e4%b8%8e%e4%b8%bb%e5%8a%a8%e9%98%b2%e5%be%a1%e7%9a%84%e5%8c%ba%e5%88%ab></a>被动防御与主动防御的区别</h3><p>被动防御是在不改变模型的前提下，找到对抗样本，而主动防御是训练一个能够抵抗攻击的模型。</p></div><footer class=article-footer><div class=share-wrapper><a href="http://connect.qq.com/widget/shareqq/index.html?url=https://user-xixiboliya.github.io/post/ai_system%E5%A4%8D%E4%B9%A0/&amp;title=2025XJTU%20AI%e7%b3%bb%e7%bb%9f%e5%a4%8d%e4%b9%a0&amp;desc=%e6%9c%ac%e8%af%be%e7%a8%8b%e7%9a%84%e4%b8%ad%e6%96%87%e5%90%8d%e7%a7%b0%e8%ae%be%e5%ae%9a%e4%b8%ba%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e7%b3%bb%e7%bb%9f%ef%bc%8c%e4%b8%bb%e8%a6%81%e8%ae%b2%e8%a7%a3%e6%94%af%e6%8c%81%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e7%9a%84%e8%ae%a1%e7%ae%97%e6%9c%ba%e7%b3%bb%e7%bb%9f%e8%ae%be%e8%ae%a1%ef%bc%8c%e5%af%b9%e5%ba%94%e7%9a%84%e8%8b%b1%e6%96%87%e8%af%be%e7%a8%8b%e5%90%8d%e7%a7%b0%e4%b8%ba%20System%20for%20AI%e3%80%82%e6%9c%ac%e8%af%be%e7%a8%8b%e4%b8%ad%e5%b0%86%e4%ba%a4%e6%9b%bf%e4%bd%bf%e7%94%a8%e4%bb%a5%e4%b8%8b%e8%af%8d%e6%b1%87%ef%bc%9a%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e7%b3%bb%e7%bb%9f%ef%bc%8cAI-System%20%e5%92%8c%20System%20for%20AI%e3%80%82%e6%9c%ac%e8%af%be%e7%a8%8b%e4%b8%ba%e5%be%ae%e8%bd%af%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e6%95%99%e8%82%b2%e4%b8%8e%e5%85%b1%e5%bb%ba%e7%a4%be%e5%8c%ba%e4%b8%ad%e8%a7%84%e5%88%92%e7%9a%84%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e7%9b%b8%e5%85%b3%e6%95%99%e7%a8%8b%e4%b9%8b%e4%b8%80%ef%bc%8c%e5%9c%a8%e5%9f%ba%e7%a1%80%e6%95%99%e7%a8%8b%e6%a8%a1%e5%9d%97%e4%b8%8b%ef%bc%8c%e8%af%be%e7%a8%8b%e7%bc%96%e5%8f%b7%e5%92%8c%e5%90%8d%e7%a7%b0%e4%b8%ba%20A6-%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e7%b3%bb%e7%bb%9f%e3%80%82&amp;source=https://user-xixiboliya.github.io/" target=_blank rel="noopener noreferrer" title="2025XJTU AI系统复习"><div class="share-icon icon icon-qq"></div></a><a href=javascript:; title="2025XJTU AI系统复习"><div class="share-icon icon icon-weixin"><div id=share-weixin><div class=share-weixin-dom><div class=share-weixin-content><img id=share-weixin-banner><div id=share-weixin-title></div><div id=share-weixin-desc></div></div><div class=share-weixin-qrcode><div class=share-weixin-info><div id=share-weixin-author></div><div id=share-weixin-theme>Powered By hugo-theme-reimu</div></div><img id=share-weixin-qr></div></div><div class=share-weixin-canvas></div></div></div></a></div><ul class=article-tag-list itemprop=keywords></ul></footer></div><nav id=article-nav data-aos=fade-up><div class="article-nav-link-wrap article-nav-link-left"><img data-src="https://upload-bbs.miyoushe.com/upload/2025/04/06/289087756/385d85817339579eeed90723586bd475_8928971233457410094.png?x-oss-process=image//resize,s_600/quality,q_80/auto-orient,0/interlace,1/format,png" data-sizes=auto alt="2025XJTU AI系统实验" class=lazyload>
<a href=https://user-xixiboliya.github.io/post/ai_system/></a><div class=article-nav-caption>Newer</div><h3 class=article-nav-title>2025XJTU AI系统实验</h3></div><div class="article-nav-link-wrap article-nav-link-right"><img data-src="https://upload-bbs.miyoushe.com/upload/2024/08/14/225492857/b97564ca9f720a03826357b782b5a66d_7132709697266664689.png?x-oss-process=image//resize,s_600/quality,q_80/auto-orient,0/interlace,1/format,png" data-sizes=auto alt="RC 网络仓库服务器" class=lazyload>
<a href=https://user-xixiboliya.github.io/post/rcserver/></a><div class=article-nav-caption>Older</div><h3 class=article-nav-title>RC 网络仓库服务器</h3></div></nav></article></section></div><footer id=footer><div style=width:100%;overflow:hidden><div class=footer-line></div></div><div id=footer-info><div><span class=icon-copyright></span>
2020 -
2025
<span class="footer-info-sep rotate"></span>
Bertsin</div><div>Powered by&nbsp;<a href=https://gohugo.io/ target=_blank>Hugo</a>&nbsp; Theme.<a href=https://github.com/D-Sketon/hugo-theme-reimu target=_blank>Reimu</a></div><div><span class=icon-brush>&nbsp;
11.4k
</span>&nbsp;|&nbsp;
<span class=icon-coffee>&nbsp;
01:10</span></div><div><span class=icon-eye></span>
<span id=busuanzi_container_site_pv>Number of visits&nbsp;<span id=busuanzi_value_site_pv></span></span>
&nbsp;|&nbsp;
<span class=icon-user></span>
<span id=busuanzi_container_site_uv>Number of visitors&nbsp;<span id=busuanzi_value_site_uv></span></span></div></div></footer><div class=sidebar-top><div class="sidebar-top-taichi rotate"></div><div class=arrow-up></div></div><div id=mask class=hide></div></div><nav id=mobile-nav><div class=sidebar-wrap><div class=sidebar-toc-sidebar><div class=sidebar-toc><h3 class=toc-title>Contents</h3><div class="sidebar-toc-wrapper toc-div-class"><nav id=TableOfContents><ul><li><a href=#早期的深度学习框架>早期的深度学习框架</a><ul><li><a href=#基于数据流图dag的计算框架>基于数据流图DAG的计算框架</a></li><li><a href=#自动求导>自动求导</a></li><li><a href=#图优化>图优化</a></li></ul></li></ul><ul><li><ul><li><a href=#池化操作>池化操作</a></li><li><a href=#pytorch示例代码>Pytorch示例代码</a></li><li><a href=#inception-module>Inception Module</a></li></ul></li><li><a href=#加载torchvision的自带数据集>加载torchvision的自带数据集</a></li><li><a href=#定制数据集>定制数据集</a><ul><li><a href=#生成定制的dataset>生成定制的dataset</a></li><li><a href=#生成定制的transform>生成定制的transform</a></li></ul></li><li><a href=#linux服务器和vim>linux、服务器和vim</a><ul><li><a href=#服务器框架与环境配置>服务器框架与环境配置</a></li></ul></li></ul><ul><li><a href=#卷积层映射到矩阵运算>卷积层映射到矩阵运算</a></li><li><a href=#cpu体系结构>CPU体系结构</a></li><li><a href=#并行处理硬件架构>并行处理硬件架构</a><ul><li><a href=#sisd单指令流单数据流>SISD单指令流单数据流</a></li><li><a href=#simd单指令流多数据流single-instructionmultiple-data>SIMD单指令流多数据流(Single Instruction,Multiple Data)</a></li></ul></li><li><a href=#矩阵乘的优化方案>矩阵乘的优化方案</a></li><li><a href=#gpu体系结构>GPU体系结构</a><ul><li><a href=#gpu执行模型>GPU执行模型</a></li><li><a href=#gpu内存架构>GPU内存架构</a></li><li><a href=#矩阵运算专用芯片asics>矩阵运算专用芯片ASICs</a></li><li><a href=#节省访存的核心脉动阵列>节省访存的核心：脉动阵列</a></li></ul></li></ul><ul><li><a href=#一般编译器>一般编译器</a></li><li><a href=#ai编译器的基本构成>AI编译器的基本构成</a></li><li><a href=#gcc与llvm>GCC与LLVM</a></li><li><a href=#神经网络编译器>神经网络编译器</a><ul><li><a href=#中间表示计算图>中间表示——计算图</a></li><li><a href=#前端优化>前端优化</a></li></ul></li><li><a href=#后端优化>后端优化</a></li></ul><ul><li><a href=#权重稀疏>权重稀疏</a><ul><li><a href=#非结构化剪枝>非结构化剪枝</a></li><li><a href=#结构化剪枝>结构化剪枝</a></li><li><a href=#正则化>正则化</a></li></ul></li><li><a href=#激活稀疏>激活稀疏</a></li><li><a href=#梯度稀疏>梯度稀疏</a></li><li><a href=#量化低比特计算>量化/低比特计算</a></li><li><a href=#三值网络二值网络>三值网络/二值网络</a></li><li><a href=#轻量化网络>轻量化网络</a><ul><li><a href=#模型复杂度分析>模型复杂度分析</a></li></ul></li></ul><ul><li><a href=#算子内并行>算子内并行</a></li><li><a href=#算子间并行>算子间并行</a></li><li><a href=#模型并行>模型并行</a><ul><li><a href=#模型并行的优化措施流水线>模型并行的优化措施——流水线</a></li><li><a href=#gpipe算法>Gpipe算法</a></li><li><a href=#pipedream算法>PipeDream算法</a></li></ul></li><li><a href=#数据并行>数据并行</a><ul><li><a href=#一对多>一对多</a></li><li><a href=#多对一>多对一</a></li><li><a href=#多对多>多对多</a></li><li><a href=#allreduce-的通信原语实现算法1>Allreduce 的通信原语实现算法1</a></li><li><a href=#allreduce-的通信原语实现算法2>Allreduce 的通信原语实现算法2</a></li><li><a href=#ring-allreduce>Ring Allreduce</a></li><li><a href=#通信后端>通信后端</a></li><li><a href=#集合通信原语的应用分布式sgd算法>集合通信原语的应用：分布式SGD算法</a></li><li><a href=#单机多卡分布式sgd>单机多卡分布式SGD</a></li><li><a href=#多机分布式sgd>多机分布式SGD</a></li><li><a href=#具有同步障的dist-sgd算法>具有同步障的dist-SGD算法</a></li><li><a href=#horovod>Horovod</a></li></ul></li><li><a href=#小结>小结</a></li></ul><ul><li><a href=#docker的介绍>Docker的介绍</a></li><li><a href=#镜像与容器实践>镜像与容器实践</a></li><li><a href=#公平性调度>公平性调度</a><ul><li><a href=#异构资源的公平性调度>异构资源的公平性调度</a></li><li><a href=#拓扑亲和性>拓扑亲和性</a></li><li><a href=#hived调度算法>HiveD调度算法</a></li></ul></li><li><a href=#调度的灵活性弹性与抢占>调度的灵活性——弹性与抢占</a><ul><li><a href=#hadoop-yarn抢占的实现>Hadoop YARN抢占的实现</a></li></ul></li><li><a href=#异构计算集群管理系统>异构计算集群管理系统</a></li></ul><ul><li><a href=#示例dockerfile>示例dockerfile</a></li><li><a href=#docker部署pytorch推理程序>docker部署pytorch推理程序</a></li></ul><ul><li><a href=#对抗攻击>对抗攻击</a><ul><li><a href=#攻击方法fgsm>攻击方法FGSM</a></li><li><a href=#白盒与黑盒>白盒与黑盒</a></li></ul></li><li><a href=#通用对抗攻击>通用对抗攻击</a><ul><li><a href=#对抗补丁>对抗补丁</a></li></ul></li><li><a href=#防御>防御</a><ul><li><a href=#被动防御>被动防御</a></li><li><a href=#主动防御>主动防御</a></li><li><a href=#被动防御与主动防御的区别>被动防御与主动防御的区别</a></li></ul></li></ul></nav></div></div></div><div class="sidebar-common-sidebar hidden"><div class=sidebar-author><img data-src=/avatar/avatar.webp data-sizes=auto alt=Bertsin class=lazyload><div class=sidebar-author-name>Bertsin</div><div class=sidebar-description>祈祷中...</div></div><div class=sidebar-state><div class=sidebar-state-article><div>Posts</div><div class=sidebar-state-number>24</div></div><div class=sidebar-state-category><div>Categories</div><div class=sidebar-state-number>8</div></div><div class=sidebar-state-tag><div>Tags</div><div class=sidebar-state-number>1</div></div></div><div class=sidebar-social><div class="icon-bilibili sidebar-social-icon"><a href="https://space.bilibili.com/651491932?spm_id_from=333.1007.0.0" itemprop=url target=_blank aria-label=bilibili rel="noopener external nofollow noreferrer"></a></div><div class="icon-email sidebar-social-icon"><a href=linboxi123@163.com itemprop=url target=_blank aria-label=email rel="noopener external nofollow noreferrer"></a></div><div class="icon-github sidebar-social-icon"><a href=https://github.com/user-xixiboliya itemprop=url target=_blank aria-label=github rel="noopener external nofollow noreferrer"></a></div><div class="icon-zhihu sidebar-social-icon"><a href=https://www.zhihu.com/people/lllll-19-64-21 itemprop=url target=_blank aria-label=zhihu rel="noopener external nofollow noreferrer"></a></div></div><div class=sidebar-menu><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/ aria-label=Home></a><div class='sidebar-menu-icon icon rotate'>&#xe62b;</div><div class=sidebar-menu-link>Home</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/archives aria-label=Archives></a><div class='sidebar-menu-icon icon'>&#xe633;</div><div class=sidebar-menu-link>Archives</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/about aria-label=About></a><div class='sidebar-menu-icon icon'>&#xe63d;</div><div class=sidebar-menu-link>About</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/friend aria-label=Friend></a><div class='sidebar-menu-icon icon'>&#xe639;</div><div class=sidebar-menu-link>Friend</div></div></div></div></div><div class=sidebar-btn-wrapper><div class="sidebar-toc-btn current"></div><div class=sidebar-common-btn></div></div></nav></div><div class=site-search><div class="reimu-popup popup"><div class=reimu-search><div class=reimu-search-input-icon></div><div class=reimu-search-input id=reimu-search-input></div><div class=popup-btn-close></div></div><div class=reimu-results><div id=reimu-stats></div><div id=reimu-hits></div><div id=reimu-pagination class=reimu-pagination></div></div><img class=reimu-bg src=/images/reimu.png></div></div><script src=https://npm.webcache.cn/lazysizes@5.3.2/lazysizes.min.js integrity=sha384-3gT/vsepWkfz/ff7PpWNUeMzeWoH3cDhm/A8jM7ouoAK0/fP/9bcHHR5kHq2nf+e crossorigin=anonymous></script><script src=https://npm.webcache.cn/clipboard@2.0.11/dist/clipboard.min.js integrity=sha384-J08i8An/QeARD9ExYpvphB8BsyOj3Gh2TSh1aLINKO3L0cMSH2dN3E22zFoXEi0Q crossorigin=anonymous></script><script src=/js/main.js integrity crossorigin=anonymous></script><script src=/js/aos.js integrity crossorigin=anonymous></script><script>var aosInit=()=>{AOS.init({duration:1e3,easing:"ease",once:!0,offset:50})};document.readyState==="loading"?document.addEventListener("DOMContentLoaded",aosInit):aosInit()</script><script src=/js/pjax_main.js integrity crossorigin=anonymous data-pjax></script><script>var ALGOLIA_CONFIG={logo:"/images/algolia_logo.svg",algolia:{applicationID:"E7TXX1S76T",apiKey:"f3def5697bab431015815d63d4289527",indexName:"algolia",hits:{per_page:parseInt("10")},labels:{input_placeholder:"搜索.....",hits_empty:"未发现与 「${query}」相关内容",hits_stats:"找到${hits}条结果（用时 ${time} ms）"}}}</script><script src=https://npm.webcache.cn/algoliasearch@4.17.1/dist/algoliasearch-lite.umd.js defer integrity=sha384-xvLS0jfKuoREs7pqkRI/OI8GcqohO5S+jQz7ZBtQXnsXmD+9jDOOY4cL6dCPzlrk crossorigin=anonymous></script><script src=https://npm.webcache.cn/instantsearch.js@4.56.1/dist/instantsearch.production.min.js defer integrity=sha384-hHJCflT4KBLQyHfKO9vpstIcFKn/Y+KHTORelMMEn7mOp2AVPp+7fr03dLgZiV3J crossorigin=anonymous></script><script src=/js/algolia_search.js integrity crossorigin=anonymous></script><div id=lazy-script><div><script data-pjax>window.REIMU_POST={author:"Bertsin",title:"2025XJTU AI系统复习",url:"https://user-xixiboliya.github.io/post/ai_system%E5%A4%8D%E4%B9%A0/",description:"本课程的中文名称设定为人工智能系统，主要讲解支持人工智能的计算机系统设计，对应的英文课程名称为 System for AI。本课程中将交替使用以下词汇：人工智能系统，AI-System 和 System for AI。本课程为微软人工智能教育与共建社区中规划的人工智能相关教程之一，在基础教程模块下，课程编号和名称为 A6-人工智能系统。",cover:"https://user-xixiboliya.github.io/images/banner.webp"}</script><script src=/js/insert_highlight.js integrity crossorigin=anonymous data-pjax></script><script type=module data-pjax>
        const PhotoSwipeLightbox = (await safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe-lightbox.esm.min.js", "sha384-DiL6M\/gG\u002bwmTxmCRZyD1zee6lIhawn5TGvED0FOh7fXcN9B0aZ9dexSF\/N6lrZi\/")).default;

        const pswp = () => {
          if (_$$('.article-entry a.article-gallery-item').length > 0) {
            new PhotoSwipeLightbox({
              gallery: '.article-entry',
              children: 'a.article-gallery-item',
              pswpModule: () => safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js", "sha384-WkkO3GCmgkC3VQWpaV8DqhKJqpzpF9JoByxDmnV8\u002boTJ7m3DfYEWX1fu1scuS4\u002bs")
            }).init();
          }
          if(_$$('.article-gallery a.article-gallery-item').length > 0) {
            new PhotoSwipeLightbox({
              gallery: '.article-gallery',
              children: 'a.article-gallery-item',
              pswpModule: () => safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js", "sha384-WkkO3GCmgkC3VQWpaV8DqhKJqpzpF9JoByxDmnV8\u002boTJ7m3DfYEWX1fu1scuS4\u002bs")
            }).init();
          }
          window.lightboxStatus = 'done';
          window.removeEventListener('lightbox:ready', pswp);
        }
        if(window.lightboxStatus === 'ready') {
          pswp()
        } else {
          window.addEventListener('lightbox:ready', pswp);
        }
      </script><script src=https://npm.webcache.cn/qrcode@1.4.4/build/qrcode.min.js defer data-pjax integrity=sha384-0RsG1yo/crf/1Qc14sho26SXXOTngNCjgJw7fuvXBt9W/OChF/Ijx+aUuBDqQwEk crossorigin=anonymous></script><script src=https://npm.webcache.cn/html-to-image@1.11.11/dist/html-to-image.js defer data-pjax integrity=sha384-UbfRVKN3/elS1r7JcK2FhmPP+KlJ4CvYwbyYD7tH+uTkbT9bNJr9eJeQ0FoFbAgz crossorigin=anonymous></script></div></div><script src=https://npm.webcache.cn/busuanzi@2.3.0/bsz.pure.mini.js async integrity=sha384-0M75wtSkhjIInv4coYlaJU83+OypaRCIq2SukQVQX04eGTCBXJDuWAbJet56id+S crossorigin=anonymous></script><script>"serviceWorker"in navigator&&navigator.serviceWorker.getRegistrations().then(e=>{for(let t of e)t.unregister()})</script><script>const reimuCopyright=String.raw`
   ______     ______     __     __    __     __  __    
  /\  == \   /\  ___\   /\ \   /\ "-./  \   /\ \/\ \   
  \ \  __<   \ \  __\   \ \ \  \ \ \-./\ \  \ \ \_\ \  
   \ \_\ \_\  \ \_____\  \ \_\  \ \_\ \ \_\  \ \_____\ 
    \/_/ /_/   \/_____/   \/_/   \/_/  \/_/   \/_____/ 
                                                    
  `;console.log(String.raw`%c ${reimuCopyright}`,"color: #ff5252;"),console.log("%c Theme.Reimu %c https://github.com/D-Sketon/hugo-theme-reimu ","color: white; background: #ff5252; padding:5px 0;","padding:4px;border:1px solid #ff5252;")</script></body></html>