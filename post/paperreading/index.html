<!doctype html><html lang=en-us data-theme-mode=auto><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Paper Reading 1 | Bertsin
</title><meta name=description content="快速的从一些论文中扫过，您真的要dive into 它们吗？"><script>window.siteConfig=JSON.parse('{"anchor_icon":null,"clipboard":{"copyright":{"content":"本文版权：本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！","count":50,"enable":false},"fail":"复制失败 (ﾟ⊿ﾟ)ﾂ","success":"复制成功(*^▽^*)"},"code_block":{"expand":true},"icon_font":"4552607_tq6stt6tcg","outdate":{"daysago":180,"enable":false,"message":"本文最后更新于 {time}，请注意文中内容可能已经发生变化。"}}')</script><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7cNoto%20Serif%20SC:400,400italic,700,700italic%7c&amp;display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7cNoto%20Serif%20SC:400,400italic,700,700italic%7c&amp;display=swap" media=print onload='this.media="all"'><link rel=preload href=//at.alicdn.com/t/c/font_4552607_tq6stt6tcg.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=/css/loader.min.2ad0e9bbffb534e893c0ecefc44787a277cf851387e8ad9dccfbc3a5f0886dbe.css><meta property="og:type" content="website"><meta property="og:title" content="Paper Reading 1 | Bertsin"><meta property="og:description" content="快速的从一些论文中扫过，您真的要dive into 它们吗？"><meta property="og:url" content="https://user-xixiboliya.github.io/post/paperreading/"><meta property="og:site_name" content="Welcome to Bertsin Homepage!"><meta property="og:image" content="/"><meta property="article:author" content="Bertsin"><meta property="article:published_time" content="2025-10-21T15:00:00-07:10"><meta property="article:modified_time" content="2025-10-21T16:00:00-07:20"><meta name=twitter:card content="summary"><meta name=twitter:image content="/"><link rel="shortcut icon" href=/favicon.ico><link rel=stylesheet href=/css/main.min.a23591d00af1734c216ea77b270e5fab60a4f92b6498af06e6b0dde48f7c49c0.css><link rel=preload as=style href=https://npm.webcache.cn/photoswipe@5.4.4/dist/photoswipe.css onload='this.onload=null,this.rel="stylesheet"'><link rel=preload as=style href=https://npm.webcache.cn/katex@0.16.9/dist/katex.min.css onload='this.onload=null,this.rel="stylesheet"'><script src=https://npm.webcache.cn/pace-js@1.2.4/pace.min.js integrity=sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8 crossorigin=anonymous></script><link rel=stylesheet href=https://npm.webcache.cn/@reimujs/aos@0.1.0/dist/aos.css></head><body><div id=loader><div class="loading-left-bg loading-bg"></div><div class="loading-right-bg loading-bg"></div><div class=spinner-box><div class=loading-taichi><svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" shape-rendering="geometricPrecision"><path d="M303.5 432a80 80 0 01-12 160 80 80 0 0112-160z" fill="#ff5252"/><path d="M512 65a447 447 0 010 894V929a417 417 0 000-834 417 417 0 000 834v30a447 447 0 010-894zm0 30A417 417 0 01929 512 208.5 208.5.0 01720.5 720.5V592a80 80 0 000-160 80 80 0 000 160V720.5A208.5 208.5.0 01512 512 208.5 208.5.0 00303.5 303.5 208.5 208.5.0 0095 512 417 417 0 01512 95z" fill="#ff5252"/></svg></div><div class=loading-word>少女祈祷中...</div></div></div></div><script>var time=null,startLoading=()=>{time=Date.now(),document.getElementById("loader").classList.remove("loading")},endLoading=()=>{time?Date.now()-time>500?(time=null,document.body.style.overflow="auto",document.getElementById("loader").classList.add("loading")):(setTimeout(endLoading,500-(Date.now()-time)),time=null):(document.body.style.overflow="auto",document.getElementById("loader").classList.add("loading"))};window.addEventListener("DOMContentLoaded",endLoading),document.getElementById("loader").addEventListener("click",endLoading)</script><div id=copy-tooltip style="pointer-events:none;opacity:0;transition:all .2s ease;position:fixed;top:50%;left:50%;z-index:999;transform:translate(-50%,-50%);color:#fff;background:rgba(0,0,0,.5);padding:10px 15px;border-radius:10px"></div><div id=container><div id=wrap><div id=header-nav><nav id=main-nav><span class=main-nav-link-wrap><div class='main-nav-icon icon rotate'>&#xe62b;</div><a class=main-nav-link href=/>Home</a>
</span><span class=main-nav-link-wrap><div class='main-nav-icon icon'>&#xe633;</div><a class=main-nav-link href=/archives>Archives</a>
</span><span class=main-nav-link-wrap><div class='main-nav-icon icon'>&#xe63d;</div><a class=main-nav-link href=/about>About</a>
</span><span class=main-nav-link-wrap><div class='main-nav-icon icon'>&#xe639;</div><a class=main-nav-link href=/friend>Friend</a>
</span><a id=main-nav-toggle class=nav-icon></a></nav><nav id=sub-nav><a id=nav-search-btn class="nav-icon popup-trigger" title=Search></a></nav></div><header id=header><picture></picture>
<img fetchpriority=high src=/images/liyue-default.png alt="Paper Reading 1"><div id=header-outer><div id=header-title><a href=/ id=logo><h1 data-aos=slide-up>Paper Reading 1</h1></a><h2 id=subtitle-wrap data-aos=slide-down></h2></div></div></header><div id=content class=sidebar-right><aside id=sidebar><div class="sidebar-wrapper wrap-sticky"><div class=sidebar-wrap data-aos=fade-up><div class=sidebar-toc-sidebar><div class=sidebar-toc><h3 class=toc-title>Contents</h3><div class="sidebar-toc-wrapper toc-div-class"><nav id=TableOfContents><ul><li><a href=#test-time-scaling-with-world-models-for-spatial-reasoning>Test-Time Scaling with World Models for Spatial Reasoning</a></li><li><a href=#show-o>show-o</a></li><li><a href=#navila-legged-robot-vision-language-action-model-for-navigation>NaVILA: Legged Robot Vision-Language-Action Model for Navigation</a></li><li><a href=#you-only-teach-once>You Only Teach Once</a></li><li><a href=#robotdancing-residual-action-reinforcement-learning-enables-robust-long-horizon-humanoid-motion-tracking>RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking</a></li><li><a href=#iterative-rl-for-vla-model-ire-vla>Iterative RL for VLA model (iRe-VLA)</a></li><li><a href=#do-generative-video-models-learn-physical-principles-from-videos>Do generative video models learn physical principles from videos?</a></li><li><a href=#local-policies-enable-zero-shot-long-horizon-manipulation>Local Policies Enable Zero-shot Long-horizon Manipulation</a></li></ul></nav></div></div></div><div class="sidebar-common-sidebar hidden"><div class=sidebar-author><img data-src=/avatar/avatar.webp data-sizes=auto alt=Bertsin class=lazyload><div class=sidebar-author-name>Bertsin</div><div class=sidebar-description>祈祷中...</div></div><div class=sidebar-state><div class=sidebar-state-article><div>Posts</div><div class=sidebar-state-number>25</div></div><div class=sidebar-state-category><div>Categories</div><div class=sidebar-state-number>8</div></div><div class=sidebar-state-tag><div>Tags</div><div class=sidebar-state-number>1</div></div></div><div class=sidebar-social><div class="icon-bilibili sidebar-social-icon"><a href="https://space.bilibili.com/651491932?spm_id_from=333.1007.0.0" itemprop=url target=_blank aria-label=bilibili rel="noopener external nofollow noreferrer"></a></div><div class="icon-email sidebar-social-icon"><a href=linboxi123@163.com itemprop=url target=_blank aria-label=email rel="noopener external nofollow noreferrer"></a></div><div class="icon-github sidebar-social-icon"><a href=https://github.com/user-xixiboliya itemprop=url target=_blank aria-label=github rel="noopener external nofollow noreferrer"></a></div><div class="icon-zhihu sidebar-social-icon"><a href=https://www.zhihu.com/people/lllll-19-64-21 itemprop=url target=_blank aria-label=zhihu rel="noopener external nofollow noreferrer"></a></div></div><div class=sidebar-menu><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/ aria-label=Home></a><div class='sidebar-menu-icon icon rotate'>&#xe62b;</div><div class=sidebar-menu-link>Home</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/archives aria-label=Archives></a><div class='sidebar-menu-icon icon'>&#xe633;</div><div class=sidebar-menu-link>Archives</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/about aria-label=About></a><div class='sidebar-menu-icon icon'>&#xe63d;</div><div class=sidebar-menu-link>About</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/friend aria-label=Friend></a><div class='sidebar-menu-icon icon'>&#xe639;</div><div class=sidebar-menu-link>Friend</div></div></div></div><div class=sidebar-btn-wrapper style=position:static><div class="sidebar-toc-btn current"></div><div class=sidebar-common-btn></div></div></div></div><div class=sidebar-widget></div></aside><section id=main><article class="h-entry article" itemprop=blogPost itemscope itemtype=https://schema.org/BlogPosting><div class=article-inner data-aos=fade-up><div class=article-meta><div class=article-date><span class=article-date-link data-aos=zoom-in><time datetime="2025-10-21 15:00:00 -0710 -0710" itemprop=datePublished>2025-10-21</time>
<time style=display:none id=post-update-time>2025-10-21</time></span></div><div class=article-category><a class=article-category-link href=/categories/emboieded-ai data-aos=zoom-in>EMBOIEDED AI</a></div></div><div class=hr-line></div><div class="e-content article-entry" itemprop=articleBody><h1 id=pi0><a class=header-anchor href=#pi0></a>pi0</h1>$$\boldsymbol{o}_t = \left[ \left[ \boldsymbol{I}_1^t, \dots, \boldsymbol{I}_n^t, \ell_t \right], \left[ \boldsymbol{q}_t \right], \left[ \boldsymbol{a}_t^\tau, \boldsymbol{a}_{t+1}^\tau, \dots, \boldsymbol{a}_{t+H-1}^\tau \right] \right]$$<p><img src=output_image/b4cdde6c0b4bfdc0e1809b25b4a2b740.png alt></p><p>$\pi_{0}$的数据集构建的很成功，预训练的数据来源未$\pi 0$数据集以及开源数据集OXE，以量取胜，之后再对于特定任务进行微调。为了使数据结构统一，robot state被对齐到18维度。</p>$$L^\tau(\theta) = \mathbb{E}_{p(\boldsymbol{A}_t \mid \boldsymbol{o}_t), q(\boldsymbol{A}_t^\tau \mid \boldsymbol{A}_t)} \left\| \boldsymbol{v}_\theta \left( \boldsymbol{A}_t^\tau, \boldsymbol{o}_t \right) - \boldsymbol{u} \left( \boldsymbol{A}_t^\tau \mid \boldsymbol{A}_t \right) \right\|^2$$<p>在推理时，真实动作序列通过Euler步进法进行恢复,$\boldsymbol{A}_{t}^{\tau + \delta} = \boldsymbol{A}_{t}^{\tau} + \delta \boldsymbol{v}_{\theta}(\boldsymbol{A}_{t}^{\tau}, \boldsymbol{o}_{t})$，$\delta$=0.1。以下是对上式的解读，不过貌似原论文没找到。
<img src=output_image/7531c5cd6f6e9000ed73ec099d3c1b4a.png alt></p><p>原文提出的一些局限性：大多数知识来源于预训练阶段，微调则负责告诉模型如何用某些知识完成用户的指令。复杂任务的高精度数据(longhorizon移动操作任务)进行微调会导致模型脆弱，以zero shot 方式运行预训练模型并不总能展现出后训练数据中所展示的流畅策略。</p><blockquote><p>action expert 有两种，一种是自回归的detokenizer,另一种是flow matching 的Diffusion Policy（例如pi0）。流匹配的DP倾向于让model学到一个分布，将噪声流到动作，输出的控制信号是连续的。自回归的Detokenizer是逐token预测，输出空间是离散的token化动作。</p><p>对于VLA而言，大部分VLA的主干是VLM，另⼀种范式的兴起也不可忽视，也就是World Model的范式，换言之，即使用 Video Prediction Model 作为 VLA 的主干部分，并且辅佐以 DP 或者 Detokenizer 进行 Action 的输出。<a href=https://axi404.top/blog/embodied-talk-2#vlm-%E7%9A%84%E4%B8%A4%E7%A7%8D%E8%8C%83%E5%BC%8F>参考链接</a> 相关工作有MindJourney 、Unified Video Action Model和Genie Envisioner。但这个idea并不是很新的了。</p></blockquote><h1 id=pi_05><a class=header-anchor href=#pi_05></a>$\pi_{0.5}$</h1><p>揭示了一个端到端学习的robot可以执行长时间范围的灵巧操作技能。
<img src=output_image/11c455ea3a47bd1d1fe17ecfd7459766.png alt>
$\pi_{0.5}$做了subtask的划分，高层 VLM 是建模观测 + 整体 task prompt 到 subtask prompt 的分布；低层 VLA 是建模观测 +subtask prompt 到 action 的分布，并且多了action expert。参考$\pi 0$的输入，图像 patch、文本提示和连续动作 token 均使用双向注意力。</p><p>$\pi_{0.5}$的范式与$\pi_{0}$几乎差不多，且某种程度上与unified model当前的范式基本一致，VLM在训练VLA数据的过程中输出hidden state，之后使用hidden state作为condition，并且使用流匹配的DP进行action的输出。</p><blockquote><p>向前追溯，包括TinyVLA和CogACT等，均是使用相同的思路，使用VLM作为backbone，并且使用DP/flow matching进行action的输出。</p></blockquote><h2 id=test-time-scaling-with-world-models-for-spatial-reasoning><a class=header-anchor href=#test-time-scaling-with-world-models-for-spatial-reasoning></a>Test-Time Scaling with World Models for Spatial Reasoning</h2><p>当下VLM在处理诸如预测自我中心运动后场景变化这样简单的任务时常常遇到困难，这是因为它们感知二维图像，但缺乏对三维动态的内部模型。MindJourney通过将VLM与⼀个基于视频扩散的可控世界模型相结合，赋予VLM所缺失的这种能力。</p><p><img src=output_image/d33b180cb3f4c13b7a567843decc5c64.png alt></p><p>每一个初始动作action被映射成一个相机的姿态变换，一条轨迹被确定为姿态变换序列。尽管原文在原理中意识将相机姿态输入diffusion model中，但文章末尾提到diffusion model会生成许多与查询无关的图像，可见相机位姿并没有在diffusion model中起到很好的限制作用。原文之后又讲到对于轨迹空间进行剪枝，剪枝的标准是对于每次 (轨迹，图像)对创建natural language description，让VLM进行评估。</p><p><a href="https://mp.weixin.qq.com/s/t1Y2VGoEeRdaG4aW9xc8Lg?poc_token=HAnR6GijPJZtR7kLV4h3nVVIGchVgLmDTqE82ceC">Yilun Du 在采访</a>中指出（尽管个人能量函数那一块有点扯淡），他认为其他大多数任务都在使用video model生成数据或者video based planning，应该做的是使video model做高层直接作为更高层次的planner会好许多。具体来说，给定当前的一帧图像，由video model生成一段未来的视觉视频序列。根据这段预测视频，就可以推断出当前应该采取的动作。因为如果知道未来每一帧应该处于什么位置，那么规划出当前的动作其实是一个相对简单的问题，只需要通过controller进行解析。</p><h2 id=show-o><a class=header-anchor href=#show-o></a>show-o</h2><p>自回归对于text token是合理的，但其对于image prediction受限于因果注意力，而且diffusion model在image generate方面有着更优越的能力。于是本文想提出一个单一的transformer融合自回归与扩散，同时处理多模态理解和生成任务。
<img src=output_image/075f05769f8ee2a0947d7b82e04e8606.png alt>
文本仍是用llm处理，图像采用了MAGVIT-v2，将256$\times$ 256图像编码为16 $\times$ 16个离散词元。对于图像文本输入，支持自适应因果注意力和全注意力。</p><h2 id=navila-legged-robot-vision-language-action-model-for-navigation><a class=header-anchor href=#navila-legged-robot-vision-language-action-model-for-navigation></a>NaVILA: Legged Robot Vision-Language-Action Model for Navigation</h2><ul><li>研究问题：该方法使得四足能够在复杂的环境中进行VLN</li><li>解决的challenge：是人类语言到四足低级腿部低级action的转化。架构分为两层，上层是VLA：
<img src=output_image/362264c463ee73f0608d3dc4a0d73b55.png alt></li></ul><p>训练时冻结LLM，对于Vision encoder和projector进行训练，然后就是对于LLM的输入采取规范的格式，LLM生成中间级别的指令流（例如前进75cm），然后将指令转换为固定的命令速度。尽管十分工程化，但其突出了跨平台的适应性。对于不同的机器人只需要替换低级策略，无需重新训练VLM。</p><p>数据：采用的数据是youtube、模拟轨迹（R2R-CE/RxR-CE）、辅助数据（ScanQA）和通用 VQA。传感器采用雷达生成的2.5D高度图，解决透明物体（玻璃）和强光环境下的感知问题。</p><p>文章里道出：本文提升模型了对复杂指令和场景的理解，且在现有 VLN 任务中优于前人方法，单RGB输入达到了与全景视图、里程计等输入相当的效果。</p><p>开源情况：https://navila-bot.github.io/ 部分开源</p><h2 id=you-only-teach-once><a class=header-anchor href=#you-only-teach-once></a>You Only Teach Once</h2><p><img src=output_image/4e3e7bd5c5da7e6f4b3584003253f810.png alt></p><ul><li>提出了一种高效的学习范式，从视频中提取目标（单RGB多RGB）、位姿等等特征，迁移到机械臂的动作生成。</li></ul><p>视觉动作的模仿学习通过利用标注端到端学习，基于diffusion model，本文提出了BiDP，添加了motion mask，将视觉任务简化为点云。大体框架是：</p><ul><li>提取动作的3D点轨迹到2D，滤出目标物体的关键帧进行利用<a href=https://github.com/gangweiX/IGEV>IGEV-Stereo</a>升为点云，粗略估计手部夹角。</li><li>动作injection：将轨迹简化为关键帧（检测末端执行器速度极值），手眼变换成K个动作，K为10。</li><li>手臂协调需要掩码，对于拉抽屉而言需要hold one hand. （这没什么）</li><li>现实中对物体的点云进行平移旋转达到数据增殖</li><li>将观测点云降采样，diffusion: SIM框架、DDPM和Unet等组件，未详细道出。
实验部分进行的任务都是长序列的复杂任务，相较于pi0，成本更低，自动回放（对每个任务设置不同位置和不同资产进行替代？）生成了许多演示使得数据成本极低。在长期性能比较的定量结果上有显著提升，成功率为35.0%高于EquiBot。在基础五项任务上是76.8%。</li><li>局限性：<ul><li>面向固定工作台。</li><li>执行器限制：平行夹爪无法完成精细操作。</li><li>进行了分布外的物体的测试，但任务结构相同，未涉及任务结构的变换，视觉误差和固定工作台限制其范化能力，有很多组件都有待升级。</li></ul></li></ul><h2 id=robotdancing-residual-action-reinforcement-learning-enables-robust-long-horizon-humanoid-motion-tracking><a class=header-anchor href=#robotdancing-residual-action-reinforcement-learning-enables-robust-long-horizon-humanoid-motion-tracking></a>RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking</h2><p>这是一个tiny的工作。
<img src=output_image/8234d19edb62a15dff0003286265f86e.png alt></p><ul><li>解决问题：实现长时程高动态的人形运动追踪，因为关节的电机指令并未起到闭环补偿模型与现实系统之间的Gap。</li><li>模型部分：<ul><li>舞蹈动作由LAFAN1 舞蹈集并且利用Unitree的重定向代码生成参考舞蹈动作$G$和base link position.</li><li>training：对于t时刻，从目标舞蹈动作$G$检索$t+1$时刻的动作用于纠正actor生成的action。将t时刻之前$k$步的参考动作$G_{t-k+1:t+1}$和加噪的前k步观测给到actor，用$G_{t+1}$纠正actor output，由PD转换为底层关节controller给到robot。</li><li>policy消融实验体现了论文方法收敛速度更快，能在train阶段达到更高的reward，并且误差比baseline小。</li></ul></li><li>在宇树的H2 H1 G1上泛化性良好，其他的不好说。</li></ul><h2 id=iterative-rl-for-vla-model-ire-vla><a class=header-anchor href=#iterative-rl-for-vla-model-ire-vla></a>Iterative RL for VLA model (iRe-VLA)</h2><p>解决问题：通过监督微调可以将VLMs融入低级机器人的控制，VLA的在线RL用于大型VLA可能极其不稳定，因而提出了迭代RL，即本文iRe-vla，在RL和监督微调（SFT）之间迭代。
<img src=output_image/bb1be728cc95d7a945ed99cbd4690439.png alt></p><ul><li>模型结构：<ul><li>由预训练VLM加上轻量动作头生成底层控制动作，LoRA微调VLM的参数$\theta$与动作头参数$\phi$即可。</li><li>在RL和SFT之间迭代：在RL时冻结VLM的$\theta$防止崩溃，只优化动作头（Token Learner 和 MLP）的参数$\phi$，将调出来的轨迹收集到online Database中；采用$D_{b}$内原有数据与新的在线数据$D_{RL}$监督训练，文章说是防止遗忘。</li></ul></li><li>实验部分最初在 MetaWorld 和 FrankaKitchen 基准测试中进行了实验。采用三个领域进行测试：专家数据集中观察到的任务、运用强化学习的新任务，以及留出法未见的任务。 观察曲线可以看到RL阶段Unfreeze VLM出现了性能下降。</li><li>将 iRe-VLA 方法与标准的 PPO 算法进行了比较，确保了实验的公平性。通过对比，结果表明 iRe-VLA 方法在各种任务中表现出了较好的性能。标准PPO在引入RL任务时往往不稳定。
<img src=output_image/85d8c067eab9095906f284d3bff59d1b.png alt></li><li>消融实验表明，始终冻结VLM进行RL-SFT并没有本文iRe-VLA效果好。</li><li>局限：无法从零开始学习陌生技能，当前方法更适用于已有技能的微调和优化，而非完全的技能迁移或自主学习新任务。</li></ul><h2 id=do-generative-video-models-learn-physical-principles-from-videos><a class=header-anchor href=#do-generative-video-models-learn-physical-principles-from-videos></a>Do generative video models learn physical principles from videos?</h2><p>本文是Google DeepMind的一篇报告，其主页甚至给出了各个模型对于物理理解的Leaderboard，目前在该Benchmark下最好的是VideoPoet，Sora(i2v)的rank并不是很高。</p><ul><li>论文的结论是：对视频生成式模型，视觉逼真并不等同于物理理解。</li><li>本文开发了Physics-IQ Benchmark，用于测评模型的物理理解能力。
<img src=output_image/3cfa4678aa8b5d89da066d50b4a4fe2f.png alt>
模型将看到3秒的图像并且对于接下来5秒的视频进行延续。
<img src=output_image/a46d0e89e1691686a20a74c50906a40c.png alt></li></ul><h2 id=local-policies-enable-zero-shot-long-horizon-manipulation><a class=header-anchor href=#local-policies-enable-zero-shot-long-horizon-manipulation></a>Local Policies Enable Zero-shot Long-horizon Manipulation</h2><ul><li>问题：sim2real中面临挑战，主要源于难以模拟复杂的接触以及“生成真实的任务分布”。</li><li>贡献：ManipGen，其利用一种新的局部策略，使得sim2real能够迁移以及zero-shot。在仿真中训练agent便可以实现迁移规模化，提出了在仿真中训练的policy；进而同VLM和运动规划器结合进行部署。
在仿真中训练了数千个RL expert，采用PPO，通过DAgger将单任务RL expert蒸馏为用于迁移的视觉动作policy，使模型能够跨物体适应多种技能。VLM将动作分解实现基于text的长时程操作以及局部的sim2real迁移。
<img src=output_image/938a7a1383901f3964d24c8128ed23bf.png alt>
对于语言目标$g$和观测$O$，使得VLM预测$k$个语言子目标$\sum\limits_k {{g_k}}$，从$g_{k}$中提取子策略，子目标被LLM结构化为(物体，技能)元组。每次的输入为第$k$子目标$g_{k}$和当前观测$O^t$，操作分为两个阶段$\pi_{reach}$和$\pi_{loc}$，$\pi_{reach}$需要接近物体并靠近目标位姿$X_{targ,k}$(采用Grounded SAM对点云分割)，运动规划器则是Neural MP。接着由$\pi_{loc}$进行丰富的交互。如此分配，不同场景下的相同任务$\pi_{loc}$相对固定，仅需观察交互区域周围的环境，采用的是腕部相机的深度图。</li></ul><p>对于上文中提到的训练RL expert的阶段一，聚焦于抓取、放置、打开、关闭把手的基础任务，基础数据是使用 3.5K 个多样化对象（UnidexGrasp 数据集），随机生成初始位姿与障碍物；以及2.6K 个门/抽屉把手（PartNet 数据集），随机化尺寸、形状、摩擦系数等物理属性。观测$O$则是单一观测，并引入特权信息（物体的网格进行位姿采样）。奖励function则综合了：特定末端执行器的位姿+特定关节配置和特定物体位姿+末端执行器相对于物体的运动+是否成功+惩罚动作。</p><p>RL expert的阶段二如何蒸馏，传统 DAgger 在多任务场景下不稳定（不同任务的初始状态差异大），改进方法为回放缓冲区（存储最近 K$\times$B 条轨迹），交替进行策略更新和与新数据收集，平衡在线与离线学习。换而言之就是边训练边采样，每采到一批新轨迹，就在当前经验池$K$批上训练一次，如果经验池满了则删去最旧的，再放入最新的，然后训练一次。$K=100$时效果最佳。除此之外对于边缘像素丢失进行了处理。</p><p>论文对于现有方法的分析是：SayCan在初始位姿不理想或者任务需要丰富的接触控制时不理想；LLMTrajGen在避障上做的不好。传统的sim2real 方法如 Transic，IndustReal 需任务特定训练或真实世界修正数据，无法 zero shot。</p><p>未来：解决透明/反光物体的深度感知问题（如 RGB-D 融合）；引入在线自适应机制（如实时策略微调）减少模块化系统的级联错误。</p></div><footer class=article-footer><div class=share-wrapper><a href="http://connect.qq.com/widget/shareqq/index.html?url=https://user-xixiboliya.github.io/post/paperreading/&amp;title=Paper%20Reading%201&amp;desc=%e5%bf%ab%e9%80%9f%e7%9a%84%e4%bb%8e%e4%b8%80%e4%ba%9b%e8%ae%ba%e6%96%87%e4%b8%ad%e6%89%ab%e8%bf%87%ef%bc%8c%e6%82%a8%e7%9c%9f%e7%9a%84%e8%a6%81dive%20into%20%e5%ae%83%e4%bb%ac%e5%90%97%ef%bc%9f&amp;source=https://user-xixiboliya.github.io/" target=_blank rel="noopener noreferrer" title="Paper Reading 1"><div class="share-icon icon icon-qq"></div></a><a href=javascript:; title="Paper Reading 1"><div class="share-icon icon icon-weixin"><div id=share-weixin><div class=share-weixin-dom><div class=share-weixin-content><img id=share-weixin-banner><div id=share-weixin-title></div><div id=share-weixin-desc></div></div><div class=share-weixin-qrcode><div class=share-weixin-info><div id=share-weixin-author></div><div id=share-weixin-theme>Powered By hugo-theme-reimu</div></div><img id=share-weixin-qr></div></div><div class=share-weixin-canvas></div></div></div></a></div><ul class=article-tag-list itemprop=keywords></ul></footer></div><nav id=article-nav data-aos=fade-up><div class="article-nav-link-wrap article-nav-link-left"><img data-src="https://upload-bbs.miyoushe.com/upload/2025/01/18/289087756/ebf7f68318ab42be975f0f4a785a18e0_947852018447306930.png?x-oss-process=image//resize,s_600/quality,q_80/auto-orient,0/interlace,1/format,png" data-sizes=auto alt="Paper Reading 2" class=lazyload>
<a href=https://user-xixiboliya.github.io/post/paperreading2/></a><div class=article-nav-caption>Newer</div><h3 class=article-nav-title>Paper Reading 2</h3></div><div class="article-nav-link-wrap article-nav-link-right"><img data-src="https://upload-bbs.miyoushe.com/upload/2023/09/04/326552234/ea7ecf7c33950c4db8f66e1db7b47361_7238569038861772413.png?x-oss-process=image//resize,s_600/quality,q_80/auto-orient,0/interlace,1/format,png" data-sizes=auto alt="Paper Reading 3 - VLN Section" class=lazyload>
<a href=https://user-xixiboliya.github.io/post/paperreading3/></a><div class=article-nav-caption>Older</div><h3 class=article-nav-title>Paper Reading 3 - VLN Section</h3></div></nav></article></section></div><footer id=footer><div style=width:100%;overflow:hidden><div class=footer-line></div></div><div id=footer-info><div><span class=icon-copyright></span>
2020 -
2025
<span class="footer-info-sep rotate"></span>
Bertsin</div><div>Powered by&nbsp;<a href=https://gohugo.io/ target=_blank>Hugo</a>&nbsp; Theme.<a href=https://github.com/D-Sketon/hugo-theme-reimu target=_blank>Reimu</a></div><div><span class=icon-brush>&nbsp;
11.5k
</span>&nbsp;|&nbsp;
<span class=icon-coffee>&nbsp;
01:11</span></div><div><span class=icon-eye></span>
<span id=busuanzi_container_site_pv>Number of visits&nbsp;<span id=busuanzi_value_site_pv></span></span>
&nbsp;|&nbsp;
<span class=icon-user></span>
<span id=busuanzi_container_site_uv>Number of visitors&nbsp;<span id=busuanzi_value_site_uv></span></span></div></div></footer><div class=sidebar-top><div class="sidebar-top-taichi rotate"></div><div class=arrow-up></div></div><div id=mask class=hide></div></div><nav id=mobile-nav><div class=sidebar-wrap><div class=sidebar-toc-sidebar><div class=sidebar-toc><h3 class=toc-title>Contents</h3><div class="sidebar-toc-wrapper toc-div-class"><nav id=TableOfContents><ul><li><a href=#test-time-scaling-with-world-models-for-spatial-reasoning>Test-Time Scaling with World Models for Spatial Reasoning</a></li><li><a href=#show-o>show-o</a></li><li><a href=#navila-legged-robot-vision-language-action-model-for-navigation>NaVILA: Legged Robot Vision-Language-Action Model for Navigation</a></li><li><a href=#you-only-teach-once>You Only Teach Once</a></li><li><a href=#robotdancing-residual-action-reinforcement-learning-enables-robust-long-horizon-humanoid-motion-tracking>RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking</a></li><li><a href=#iterative-rl-for-vla-model-ire-vla>Iterative RL for VLA model (iRe-VLA)</a></li><li><a href=#do-generative-video-models-learn-physical-principles-from-videos>Do generative video models learn physical principles from videos?</a></li><li><a href=#local-policies-enable-zero-shot-long-horizon-manipulation>Local Policies Enable Zero-shot Long-horizon Manipulation</a></li></ul></nav></div></div></div><div class="sidebar-common-sidebar hidden"><div class=sidebar-author><img data-src=/avatar/avatar.webp data-sizes=auto alt=Bertsin class=lazyload><div class=sidebar-author-name>Bertsin</div><div class=sidebar-description>祈祷中...</div></div><div class=sidebar-state><div class=sidebar-state-article><div>Posts</div><div class=sidebar-state-number>25</div></div><div class=sidebar-state-category><div>Categories</div><div class=sidebar-state-number>8</div></div><div class=sidebar-state-tag><div>Tags</div><div class=sidebar-state-number>1</div></div></div><div class=sidebar-social><div class="icon-bilibili sidebar-social-icon"><a href="https://space.bilibili.com/651491932?spm_id_from=333.1007.0.0" itemprop=url target=_blank aria-label=bilibili rel="noopener external nofollow noreferrer"></a></div><div class="icon-email sidebar-social-icon"><a href=linboxi123@163.com itemprop=url target=_blank aria-label=email rel="noopener external nofollow noreferrer"></a></div><div class="icon-github sidebar-social-icon"><a href=https://github.com/user-xixiboliya itemprop=url target=_blank aria-label=github rel="noopener external nofollow noreferrer"></a></div><div class="icon-zhihu sidebar-social-icon"><a href=https://www.zhihu.com/people/lllll-19-64-21 itemprop=url target=_blank aria-label=zhihu rel="noopener external nofollow noreferrer"></a></div></div><div class=sidebar-menu><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/ aria-label=Home></a><div class='sidebar-menu-icon icon rotate'>&#xe62b;</div><div class=sidebar-menu-link>Home</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/archives aria-label=Archives></a><div class='sidebar-menu-icon icon'>&#xe633;</div><div class=sidebar-menu-link>Archives</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/about aria-label=About></a><div class='sidebar-menu-icon icon'>&#xe63d;</div><div class=sidebar-menu-link>About</div></div><div class=sidebar-menu-link-wrap><a class=sidebar-menu-link-dummy href=/friend aria-label=Friend></a><div class='sidebar-menu-icon icon'>&#xe639;</div><div class=sidebar-menu-link>Friend</div></div></div></div></div><div class=sidebar-btn-wrapper><div class="sidebar-toc-btn current"></div><div class=sidebar-common-btn></div></div></nav></div><div class=site-search><div class="reimu-popup popup"><div class=reimu-search><div class=reimu-search-input-icon></div><div class=reimu-search-input id=reimu-search-input></div><div class=popup-btn-close></div></div><div class=reimu-results><div id=reimu-stats></div><div id=reimu-hits></div><div id=reimu-pagination class=reimu-pagination></div></div><img class=reimu-bg src=/images/reimu.png></div></div><script src=https://npm.webcache.cn/lazysizes@5.3.2/lazysizes.min.js integrity=sha384-3gT/vsepWkfz/ff7PpWNUeMzeWoH3cDhm/A8jM7ouoAK0/fP/9bcHHR5kHq2nf+e crossorigin=anonymous></script><script src=https://npm.webcache.cn/clipboard@2.0.11/dist/clipboard.min.js integrity=sha384-J08i8An/QeARD9ExYpvphB8BsyOj3Gh2TSh1aLINKO3L0cMSH2dN3E22zFoXEi0Q crossorigin=anonymous></script><script src=/js/main.js integrity crossorigin=anonymous></script><script src=/js/aos.js integrity crossorigin=anonymous></script><script>var aosInit=()=>{AOS.init({duration:1e3,easing:"ease",once:!0,offset:50})};document.readyState==="loading"?document.addEventListener("DOMContentLoaded",aosInit):aosInit()</script><script src=/js/pjax_main.js integrity crossorigin=anonymous data-pjax></script><script>var ALGOLIA_CONFIG={logo:"/images/algolia_logo.svg",algolia:{applicationID:"E7TXX1S76T",apiKey:"f3def5697bab431015815d63d4289527",indexName:"algolia",hits:{per_page:parseInt("10")},labels:{input_placeholder:"搜索.....",hits_empty:"未发现与 「${query}」相关内容",hits_stats:"找到${hits}条结果（用时 ${time} ms）"}}}</script><script src=https://npm.webcache.cn/algoliasearch@4.17.1/dist/algoliasearch-lite.umd.js defer integrity=sha384-xvLS0jfKuoREs7pqkRI/OI8GcqohO5S+jQz7ZBtQXnsXmD+9jDOOY4cL6dCPzlrk crossorigin=anonymous></script><script src=https://npm.webcache.cn/instantsearch.js@4.56.1/dist/instantsearch.production.min.js defer integrity=sha384-hHJCflT4KBLQyHfKO9vpstIcFKn/Y+KHTORelMMEn7mOp2AVPp+7fr03dLgZiV3J crossorigin=anonymous></script><script src=/js/algolia_search.js integrity crossorigin=anonymous></script><div id=lazy-script><div><script data-pjax>window.REIMU_POST={author:"Bertsin",title:"Paper Reading 1",url:"https://user-xixiboliya.github.io/post/paperreading/",description:"快速的从一些论文中扫过，您真的要dive into 它们吗？",cover:"https://user-xixiboliya.github.io/images/banner.webp"}</script><script src=/js/insert_highlight.js integrity crossorigin=anonymous data-pjax></script><script type=module data-pjax>
        const PhotoSwipeLightbox = (await safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe-lightbox.esm.min.js", "sha384-DiL6M\/gG\u002bwmTxmCRZyD1zee6lIhawn5TGvED0FOh7fXcN9B0aZ9dexSF\/N6lrZi\/")).default;

        const pswp = () => {
          if (_$$('.article-entry a.article-gallery-item').length > 0) {
            new PhotoSwipeLightbox({
              gallery: '.article-entry',
              children: 'a.article-gallery-item',
              pswpModule: () => safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js", "sha384-WkkO3GCmgkC3VQWpaV8DqhKJqpzpF9JoByxDmnV8\u002boTJ7m3DfYEWX1fu1scuS4\u002bs")
            }).init();
          }
          if(_$$('.article-gallery a.article-gallery-item').length > 0) {
            new PhotoSwipeLightbox({
              gallery: '.article-gallery',
              children: 'a.article-gallery-item',
              pswpModule: () => safeImport("https:\/\/npm.webcache.cn\/photoswipe@5.4.4\/dist\/photoswipe.esm.min.js", "sha384-WkkO3GCmgkC3VQWpaV8DqhKJqpzpF9JoByxDmnV8\u002boTJ7m3DfYEWX1fu1scuS4\u002bs")
            }).init();
          }
          window.lightboxStatus = 'done';
          window.removeEventListener('lightbox:ready', pswp);
        }
        if(window.lightboxStatus === 'ready') {
          pswp()
        } else {
          window.addEventListener('lightbox:ready', pswp);
        }
      </script><script src=https://npm.webcache.cn/qrcode@1.4.4/build/qrcode.min.js defer data-pjax integrity=sha384-0RsG1yo/crf/1Qc14sho26SXXOTngNCjgJw7fuvXBt9W/OChF/Ijx+aUuBDqQwEk crossorigin=anonymous></script><script src=https://npm.webcache.cn/html-to-image@1.11.11/dist/html-to-image.js defer data-pjax integrity=sha384-UbfRVKN3/elS1r7JcK2FhmPP+KlJ4CvYwbyYD7tH+uTkbT9bNJr9eJeQ0FoFbAgz crossorigin=anonymous></script><script data-pjax>window.MathJax={loader:{load:["input/tex","output/chtml","[tex]/ams"]},tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0},options:{processHtmlClass:"tex2jax_process"}}</script><script src=https://npm.webcache.cn/mathjax@3.2.2/es5/tex-mml-chtml.js defer data-pjax integrity=sha384-Wuix6BuhrWbjDBs24bXrjf4ZQ5aFeFWBuKkFekO2t8xFU0iNaLQfp2K6/1Nxveei crossorigin=anonymous></script></div></div><script src=https://npm.webcache.cn/busuanzi@2.3.0/bsz.pure.mini.js async integrity=sha384-0M75wtSkhjIInv4coYlaJU83+OypaRCIq2SukQVQX04eGTCBXJDuWAbJet56id+S crossorigin=anonymous></script><script>"serviceWorker"in navigator&&navigator.serviceWorker.getRegistrations().then(e=>{for(let t of e)t.unregister()})</script><script>const reimuCopyright=String.raw`
   ______     ______     __     __    __     __  __    
  /\  == \   /\  ___\   /\ \   /\ "-./  \   /\ \/\ \   
  \ \  __<   \ \  __\   \ \ \  \ \ \-./\ \  \ \ \_\ \  
   \ \_\ \_\  \ \_____\  \ \_\  \ \_\ \ \_\  \ \_____\ 
    \/_/ /_/   \/_____/   \/_/   \/_/  \/_/   \/_____/ 
                                                    
  `;console.log(String.raw`%c ${reimuCopyright}`,"color: #ff5252;"),console.log("%c Theme.Reimu %c https://github.com/D-Sketon/hugo-theme-reimu ","color: white; background: #ff5252; padding:5px 0;","padding:4px;border:1px solid #ff5252;")</script></body></html>